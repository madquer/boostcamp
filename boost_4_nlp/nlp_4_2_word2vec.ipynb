{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "golden-connectivity",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "1. 주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만든다.\n",
    "2. CBOW, Skip-gram 모델을 각각 구현한다.\n",
    "3. 모델을 학습해보고 결과를 확인한다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-image",
   "metadata": {},
   "source": [
    "* **Word Embedding - Word2Vec : CBOW(Continuous Bag-of-Words) & Skip-gram**  \n",
    "  \n",
    "  \n",
    "* **CBOW(Continuous Bag-of-Words)**  \n",
    "주변 단어들을 가지고 중심 단어를 예측하는 방식으로 학습합니다.  \n",
    "주변 단어들의 one-hot encoding 벡터를 각각 embedding layer에 projection하여 각각의 embedding 벡터를 얻고  \n",
    "이 embedding들을 element-wise한 덧셈으로 합친 뒤, 다시 linear transformation하여   \n",
    "예측하고자 하는 중심 단어의 one-hot encoding 벡터와 같은 사이즈의 벡터로 만든 뒤,   \n",
    "중심 단어의 one-hot encoding 벡터와의 loss를 계산합니다.    \n",
    "예) A cute puppy is walking in the park. & window size: 2  \n",
    "Input(주변 단어): \"A\", \"cute\", \"is\", \"walking\"  \n",
    "Output(중심 단어): \"puppy\"    \n",
    "\n",
    "\n",
    "* **Skip-gram**\n",
    "중심 단어를 가지고 주변 단어들을 예측하는 방식으로 학습합니다.   \n",
    "중심 단어의 one-hot encoding 벡터를 embedding layer에 projection하여 해당 단어의 embedding 벡터를 얻고   \n",
    "이 벡터를 다시 linear transformation하여 예측하고자 하는 각각의 주변 단어들과의 one-hot encoding 벡터와 같은 사이즈의 벡터로 만든 뒤,   \n",
    "그 주변 단어들의 one-hot encoding 벡터와의 loss를 각각 계산합니다.   \n",
    "예) A cute puppy is walking in the park. & window size: 2  \n",
    "Input(중심 단어): \"puppy\"  \n",
    "Output(주변 단어): \"A\", \"cute\", \"is\", \"walking\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-scheduling",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "freelance-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-jones",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "Word2Vec 형식에 맞게 전처리  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "toxic-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "  \"정말 맛있습니다. 추천합니다.\",\n",
    "  \"기대했던 것보단 별로였네요.\",\n",
    "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\n",
    "  \"완전 최고입니다! 재방문 의사 있습니다.\",\n",
    "  \"음식도 서비스도 다 만족스러웠습니다.\",\n",
    "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\n",
    "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\n",
    "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\n",
    "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\n",
    "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \n",
    "]\n",
    "\n",
    "test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "widespread-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affiliated-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenized(data):\n",
    "    tokenized = []\n",
    "    for sent in tqdm(data):\n",
    "        tokens  = tokenizer.morphs(sent, stem=True)\n",
    "        tokenized.append(tokens)\n",
    "        \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accepting-sacramento",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['정말', '맛있다', '.', '추천', '하다', '.'],\n",
       " ['기대하다', '것', '보단', '별로', '이다', '.'],\n",
       " ['다',\n",
       "  '좋다',\n",
       "  '가격',\n",
       "  '이',\n",
       "  '너무',\n",
       "  '비싸다',\n",
       "  '다시',\n",
       "  '가다',\n",
       "  '싶다',\n",
       "  '생각',\n",
       "  '이',\n",
       "  '안',\n",
       "  '드네',\n",
       "  '요',\n",
       "  '.'],\n",
       " ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'],\n",
       " ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'],\n",
       " ['위생',\n",
       "  '상태',\n",
       "  '가',\n",
       "  '좀',\n",
       "  '별로',\n",
       "  '이다',\n",
       "  '.',\n",
       "  '좀',\n",
       "  '더',\n",
       "  '개선',\n",
       "  '되다',\n",
       "  '기르다',\n",
       "  '바라다',\n",
       "  '.'],\n",
       " ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'],\n",
       " ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'],\n",
       " ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'],\n",
       " ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized = make_tokenized(train_data)\n",
    "train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "functioning-proof",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 94254.02it/s]\n"
     ]
    }
   ],
   "source": [
    "word_count = defaultdict(int)\n",
    "\n",
    "for tokens in tqdm(train_tokenized):\n",
    "    for token in tokens:\n",
    "        word_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "timely-actor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_count = sorted(word_count.items(), key = lambda x: x[1], reverse=True)\n",
    "print(list(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "complimentary-presence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<00:00, 544714.81it/s]\n"
     ]
    }
   ],
   "source": [
    "w2i = {} # indexing (word to index)\n",
    "for pair in tqdm(word_count):\n",
    "    if pair[0] not in w2i:\n",
    "        w2i[pair[0]] = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "following-knowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n",
      "{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n"
     ]
    }
   ],
   "source": [
    "print(train_tokenized)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-cloud",
   "metadata": {},
   "source": [
    "실제 모델에 들어가기 위한 input 을 만들기 위해 `Dataset` 클래스를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sexual-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW\n",
    "class CBOWDataset(Dataset): # 주변 단어가 input, 중심 단어가 output\n",
    "    def __init__(self, train_tokenized, window_size=2):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        \n",
    "        for tokens in tqdm(train_tokenized):\n",
    "            token_ids = [w2i[token] for token in tokens] # 각 token 을 index 로 바꾼 tokens 리스트를 반환\n",
    "            for i, id in enumerate(token_ids):\n",
    "                if i - window_size >= 0 and i + window_size < len(token_ids): # 완벽한 범위가 아닐때는 무시\n",
    "                    self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1]) # list 를 append\n",
    "                    self.y.append(id)\n",
    "                    \n",
    "        self.x = torch.LongTensor(self.x) # (전체 데이터 개수, 2 * window_size) : 완벽 범위일때만 이므로 * 2 ok\n",
    "        self.y = torch.LongTensor(self.y) # (전체 데이터 개수)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "about-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkipGram\n",
    "class SkipGramDataset(Dataset): # 중심 단어가 input, 주변 단어가 output\n",
    "    def __init__(self, train_tokenized, window_size=2):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        \n",
    "        for tokens in tqdm(train_tokenized):\n",
    "            token_ids = [w2i[token] for token in tokens]\n",
    "            for i, id in enumerate(token_ids):\n",
    "                if i - window_size >= 0 and i + window_size < len(token_ids):\n",
    "                    self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
    "                    self.x += [id] * 2 * window_size\n",
    "                    # input : x / 주변 단어마다 중심 단어를 각각 매핑 시켜버림\n",
    "                    \n",
    "        self.x = torch.LongTensor(self.x) # (전체 데이터 개수)\n",
    "        self.y = torch.LongTensor(self.y) # (전체 데이터 개수)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-possession",
   "metadata": {},
   "source": [
    "|Data type|\tdtype|\tCPU tensor|\tGPU tensor|\n",
    "|-|-|-|-|\n",
    "|32-bit floating point\t|torch.float32 or torch.float\t|torch.FloatTensor|\ttorch.cuda.FloatTensor|\n",
    "|64-bit floating point\t|torch.float64 or torch.double\t|torch.DoubleTensor|\ttorch.cuda.DoubleTensor|\n",
    "|16-bit floating point\t|torch.float16 or torch.half\t|torch.HalfTensor\t|torch.cuda.HalfTensor|\n",
    "|8-bit integer (unsigned)\t|torch.uint8\t|torch.ByteTensor\t|torch.cuda.ByteTensor|\n",
    "|8-bit integer (signed)\t|torch.int8\t|torch.CharTensor\t|torch.cuda.CharTensor|\n",
    "|16-bit integer (signed)\t|torch.int16 or torch.short\t|torch.ShortTensor\t|torch.cuda.ShortTensor|\n",
    "|32-bit integer (signed)\t|torch.int32 or torch.int\t|torch.IntTensor|\ttorch.cuda.IntTensor|\n",
    "|64-bit integer (signed)\t|torch.int64 or torch.long\t|torch.LongTensor\t|torch.cuda.LongTensor|\n",
    "|Boolean|\ttorch.bool|\ttorch.BoolTensor\t|torch.cuda.BoolTensor|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-chapel",
   "metadata": {},
   "source": [
    "각 모델에 맞는 Dataset 객체를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "coated-nancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 58497.96it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 60611.33it/s]\n"
     ]
    }
   ],
   "source": [
    "cbow_set = CBOWDataset(train_tokenized)\n",
    "skipgram_set = SkipGramDataset(train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "honest-phase",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([17, 18, 19, 10]), tensor(0)), (tensor([18,  0, 10,  0]), tensor(19)), (tensor([20, 21,  4,  2]), tensor(22)), (tensor([21, 22,  2,  0]), tensor(4)), (tensor([5, 3, 6, 7]), tensor(23)), (tensor([ 3, 23,  7, 24]), tensor(6)), (tensor([23,  6, 24, 25]), tensor(7)), (tensor([ 6,  7, 25, 26]), tensor(24)), (tensor([ 7, 24, 26, 27]), tensor(25)), (tensor([24, 25, 27, 28]), tensor(26)), (tensor([25, 26, 28,  6]), tensor(27)), (tensor([26, 27,  6, 29]), tensor(28)), (tensor([27, 28, 29, 30]), tensor(6)), (tensor([28,  6, 30, 31]), tensor(29)), (tensor([ 6, 29, 31,  0]), tensor(30)), (tensor([32, 33, 34, 35]), tensor(2)), (tensor([33,  2, 35, 11]), tensor(34)), (tensor([ 2, 34, 11, 36]), tensor(35)), (tensor([34, 35, 36, 37]), tensor(11)), (tensor([35, 11, 37,  0]), tensor(36)), (tensor([8, 1, 1, 5]), tensor(9)), (tensor([ 1,  9,  5, 38]), tensor(1)), (tensor([ 9,  1, 38,  0]), tensor(5)), (tensor([12, 39, 13,  4]), tensor(40)), (tensor([39, 40,  4,  2]), tensor(13)), (tensor([40, 13,  2,  0]), tensor(4)), (tensor([13,  4,  0, 13]), tensor(2)), (tensor([ 4,  2, 13, 14]), tensor(0)), (tensor([ 2,  0, 14, 41]), tensor(13)), (tensor([ 0, 13, 41, 42]), tensor(14)), (tensor([13, 14, 42, 43]), tensor(41)), (tensor([14, 41, 43, 44]), tensor(42)), (tensor([41, 42, 44,  0]), tensor(43)), (tensor([45,  1, 46, 47]), tensor(3)), (tensor([ 1,  3, 47,  9]), tensor(46)), (tensor([ 3, 46,  9,  1]), tensor(47)), (tensor([46, 47,  1,  7]), tensor(9)), (tensor([47,  9,  7, 48]), tensor(1)), (tensor([ 9,  1, 48,  0]), tensor(7)), (tensor([49, 15, 10,  8]), tensor(11)), (tensor([15, 11,  8,  1]), tensor(10)), (tensor([11, 10,  1, 50]), tensor(8)), (tensor([10,  8, 50,  1]), tensor(1)), (tensor([8, 1, 1, 9]), tensor(50)), (tensor([ 1, 50,  9,  1]), tensor(1)), (tensor([50,  1,  1,  5]), tensor(9)), (tensor([1, 9, 5, 3]), tensor(1)), (tensor([9, 1, 3, 0]), tensor(5)), (tensor([51, 52,  8,  6]), tensor(53)), (tensor([52, 53,  6,  7]), tensor(8)), (tensor([53,  8,  7, 54]), tensor(6)), (tensor([ 8,  6, 54,  0]), tensor(7)), (tensor([ 6,  7,  0, 55]), tensor(54)), (tensor([ 7, 54, 55, 56]), tensor(0)), (tensor([54,  0, 56,  4]), tensor(55)), (tensor([ 0, 55,  4,  2]), tensor(56)), (tensor([55, 56,  2,  0]), tensor(4)), (tensor([12, 15, 14, 57]), tensor(16)), (tensor([15, 16, 57, 58]), tensor(14)), (tensor([16, 14, 58,  3]), tensor(57)), (tensor([14, 57,  3,  0]), tensor(58)), (tensor([57, 58,  0, 16]), tensor(3)), (tensor([58,  3, 16, 59]), tensor(0)), (tensor([ 3,  0, 59,  0]), tensor(16))]\n"
     ]
    }
   ],
   "source": [
    "print(list(cbow_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "organized-union",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(0), tensor(17)), (tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(0)), (tensor(22), tensor(20)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(23), tensor(5)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(0)), (tensor(2), tensor(32)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(0)), (tensor(9), tensor(8)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(0)), (tensor(40), tensor(12)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(0)), (tensor(3), tensor(45)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(0)), (tensor(11), tensor(49)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(0)), (tensor(53), tensor(51)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(16), tensor(12)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(0))]\n"
     ]
    }
   ],
   "source": [
    "print(list(skipgram_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-binary",
   "metadata": {},
   "source": [
    "## 모델 Class 구현\n",
    "\n",
    "차례대로 두 가지 Word2Vec 모델을 구현  \n",
    "* `self.embedding` : `vocab_size` 크기의 one-hot-vector 를 특정 크기의 dim 차원으로 embedding 시키는 layer.\n",
    "* `self.linear` : 변환된 embedding vector 를 다시 원래 vocab_size 로 바꾸는 layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-assist",
   "metadata": {},
   "source": [
    "`torch.nn.Embedding`  \n",
    "```py\n",
    "torch.nn.Embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None, \n",
    "    max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, \n",
    "    sparse: bool = False, _weight: Optional[torch.Tensor] = None)\n",
    "```\n",
    "\n",
    "num_embeddings = vocab_size (단어 사이즈)  \n",
    "embedding_dim = 임베딩 차원  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "consistent-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim, sparse=True) # dim 크기의 임베딩 차원으로 임베딩\n",
    "        self.linear = nn.Linear(dim, vocab_size) # fc1\n",
    "        \n",
    "    # B : batch_size, W : Window size, d_w : word embedding size, V : vocab size\n",
    "    def forward(self, x): # x : (B, 2W) = 60개의 단어 * (window_size * 2)\n",
    "        embeddings = self.embedding(x) # (B, 2W, d_w)\n",
    "        embeddings = torch.sum(embeddings, dim=1) # (B, d_w) : 2W 차원에 맞춰 더해준다. = 주변 단어들을 하나로 통합\n",
    "        output = self.linear(embeddings) # (B, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "located-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkipGram\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
    "        self.linear = nn.Linear(dim, vocab_size)\n",
    "        \n",
    "    # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
    "    def forward(self, x): # x: (B)\n",
    "        embeddings = self.embedding(x) # (B, d_w)\n",
    "        output = self.linear(embeddings) # (B, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "understanding-filter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "specific-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = CBOW(vocab_size = len(w2i), dim=256)\n",
    "skipgram = SkipGram(vocab_size = len(w2i), dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fantastic-traffic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embedding): Embedding(60, 256, sparse=True)\n",
       "  (linear): Linear(in_features=256, out_features=60, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "casual-corpus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipGram(\n",
       "  (embedding): Embedding(60, 256, sparse=True)\n",
       "  (linear): Linear(in_features=256, out_features=60, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-kingdom",
   "metadata": {},
   "source": [
    "## 모델 학습\n",
    "\n",
    "다음과 같이 hyperparameter 를 세팅하고 DataLoader 객체를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "organized-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "learning_rate = 5e-4\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\n",
    "skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "durable-composer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[17, 18, 19, 10],\n",
       "         [18,  0, 10,  0],\n",
       "         [20, 21,  4,  2],\n",
       "         [21, 22,  2,  0]]),\n",
       " tensor([ 0, 19, 22,  4])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_loader)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-medicine",
   "metadata": {},
   "source": [
    "### CBOW 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "straight-orchestra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 1010.40it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1065.88it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1114.51it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1080.78it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1053.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Epoch : 1\n",
      "Train loss: 4.150973796844482\n",
      "Train loss: 4.4851579666137695\n",
      "Train loss: 3.2803590297698975\n",
      "Train loss: 5.0054097175598145\n",
      "Train loss: 3.5689611434936523\n",
      "Train loss: 3.519221782684326\n",
      "Train loss: 3.880596160888672\n",
      "Train loss: 4.828391075134277\n",
      "Train loss: 3.9648523330688477\n",
      "Train loss: 4.016818046569824\n",
      "Train loss: 3.8289847373962402\n",
      "Train loss: 3.7736568450927734\n",
      "Train loss: 3.3764405250549316\n",
      "Train loss: 4.224390506744385\n",
      "Train loss: 4.186812400817871\n",
      "Train loss: 3.449195623397827\n",
      "##################################################\n",
      "Epoch : 2\n",
      "Train loss: 3.9793829917907715\n",
      "Train loss: 4.3450446128845215\n",
      "Train loss: 3.1698169708251953\n",
      "Train loss: 4.8731489181518555\n",
      "Train loss: 3.4398107528686523\n",
      "Train loss: 3.2897748947143555\n",
      "Train loss: 3.7204971313476562\n",
      "Train loss: 4.703166484832764\n",
      "Train loss: 3.8560996055603027\n",
      "Train loss: 3.8547139167785645\n",
      "Train loss: 3.679326057434082\n",
      "Train loss: 3.448248863220215\n",
      "Train loss: 3.247098684310913\n",
      "Train loss: 4.125006675720215\n",
      "Train loss: 4.042225360870361\n",
      "Train loss: 3.329723596572876\n",
      "##################################################\n",
      "Epoch : 3\n",
      "Train loss: 3.813215970993042\n",
      "Train loss: 4.207294940948486\n",
      "Train loss: 3.0619583129882812\n",
      "Train loss: 4.742360591888428\n",
      "Train loss: 3.3141963481903076\n",
      "Train loss: 3.079197883605957\n",
      "Train loss: 3.566103935241699\n",
      "Train loss: 4.580610752105713\n",
      "Train loss: 3.753014087677002\n",
      "Train loss: 3.6998443603515625\n",
      "Train loss: 3.5406482219696045\n",
      "Train loss: 3.149867057800293\n",
      "Train loss: 3.120600938796997\n",
      "Train loss: 4.027894973754883\n",
      "Train loss: 3.900859832763672\n",
      "Train loss: 3.214755058288574\n",
      "##################################################\n",
      "Epoch : 4\n",
      "Train loss: 3.6525864601135254\n",
      "Train loss: 4.071929931640625\n",
      "Train loss: 2.956813335418701\n",
      "Train loss: 4.6129631996154785\n",
      "Train loss: 3.192046642303467\n",
      "Train loss: 2.8884925842285156\n",
      "Train loss: 3.4174463748931885\n",
      "Train loss: 4.460596561431885\n",
      "Train loss: 3.6552324295043945\n",
      "Train loss: 3.5524415969848633\n",
      "Train loss: 3.4126906394958496\n",
      "Train loss: 2.881004810333252\n",
      "Train loss: 2.9970340728759766\n",
      "Train loss: 3.93296217918396\n",
      "Train loss: 3.7629621028900146\n",
      "Train loss: 3.104214668273926\n",
      "##################################################\n",
      "Epoch : 5\n",
      "Train loss: 3.49759840965271\n",
      "Train loss: 3.9389750957489014\n",
      "Train loss: 2.8544068336486816\n",
      "Train loss: 4.484880447387695\n",
      "Train loss: 3.0732884407043457\n",
      "Train loss: 2.7178354263305664\n",
      "Train loss: 3.274566173553467\n",
      "Train loss: 4.342986106872559\n",
      "Train loss: 3.562258243560791\n",
      "Train loss: 3.412545680999756\n",
      "Train loss: 3.2948198318481445\n",
      "Train loss: 2.643122911453247\n",
      "Train loss: 2.8764841556549072\n",
      "Train loss: 3.840118885040283\n",
      "Train loss: 3.628777265548706\n",
      "Train loss: 2.9980127811431885\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cbow.train()\n",
    "cbow = cbow.to(device)\n",
    "optim = torch.optim.SGD(cbow.parameters(), lr = learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(1, num_epochs+1):\n",
    "    print('#' * 50)\n",
    "    print(f'Epoch : {e}')\n",
    "    for batch in tqdm(cbow_loader):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device) # (B, W), (B)\n",
    "        output = cbow(x) # (B, V)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        print(f\"Train loss: {loss.item()}\")\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-underwear",
   "metadata": {},
   "source": [
    "### SkipGram 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "confused-intelligence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 1106.55it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 1142.25it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 1277.78it/s]\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Epoch: 1\n",
      "Train loss: 4.303864479064941\n",
      "Train loss: 3.3176536560058594\n",
      "Train loss: 3.8682799339294434\n",
      "Train loss: 3.900390386581421\n",
      "Train loss: 4.208705425262451\n",
      "Train loss: 4.317439556121826\n",
      "Train loss: 3.7482657432556152\n",
      "Train loss: 3.698716640472412\n",
      "Train loss: 3.760134696960449\n",
      "Train loss: 4.153445720672607\n",
      "Train loss: 3.867478370666504\n",
      "Train loss: 3.8022804260253906\n",
      "Train loss: 3.6768083572387695\n",
      "Train loss: 4.228381156921387\n",
      "Train loss: 3.671078681945801\n",
      "Train loss: 4.348445415496826\n",
      "Train loss: 4.288251876831055\n",
      "Train loss: 3.7952089309692383\n",
      "Train loss: 3.8651225566864014\n",
      "Train loss: 3.3352270126342773\n",
      "Train loss: 3.2646682262420654\n",
      "Train loss: 3.5230817794799805\n",
      "Train loss: 3.406064987182617\n",
      "Train loss: 3.3334126472473145\n",
      "Train loss: 4.002089977264404\n",
      "Train loss: 3.348836898803711\n",
      "Train loss: 4.337854862213135\n",
      "Train loss: 4.288518905639648\n",
      "Train loss: 4.1280436515808105\n",
      "Train loss: 4.527017116546631\n",
      "Train loss: 3.7089133262634277\n",
      "Train loss: 3.409048318862915\n",
      "Train loss: 4.342926025390625\n",
      "Train loss: 3.791618824005127\n",
      "Train loss: 3.8467397689819336\n",
      "Train loss: 3.795269012451172\n",
      "Train loss: 3.678645610809326\n",
      "Train loss: 4.182173728942871\n",
      "Train loss: 3.6232619285583496\n",
      "Train loss: 4.459949493408203\n",
      "Train loss: 4.219747543334961\n",
      "Train loss: 4.155771732330322\n",
      "Train loss: 3.7661266326904297\n",
      "Train loss: 3.7894842624664307\n",
      "Train loss: 3.513296604156494\n",
      "Train loss: 2.8974828720092773\n",
      "Train loss: 3.587315797805786\n",
      "Train loss: 3.9551384449005127\n",
      "Train loss: 4.006113052368164\n",
      "Train loss: 4.310751438140869\n",
      "Train loss: 4.166032791137695\n",
      "Train loss: 3.5522656440734863\n",
      "Train loss: 3.930685043334961\n",
      "Train loss: 4.024526119232178\n",
      "Train loss: 4.061308860778809\n",
      "Train loss: 3.5870652198791504\n",
      "Train loss: 3.5889556407928467\n",
      "Train loss: 4.120203971862793\n",
      "Train loss: 4.09766960144043\n",
      "Train loss: 4.096519947052002\n",
      "Train loss: 3.871241569519043\n",
      "Train loss: 3.9818902015686035\n",
      "Train loss: 3.6411170959472656\n",
      "Train loss: 3.6543872356414795\n",
      "##################################################\n",
      "Epoch: 2\n",
      "Train loss: 4.277782917022705\n",
      "Train loss: 3.270853042602539\n",
      "Train loss: 3.8390653133392334\n",
      "Train loss: 3.854374885559082\n",
      "Train loss: 4.180195331573486\n",
      "Train loss: 4.285848140716553\n",
      "Train loss: 3.716775417327881\n",
      "Train loss: 3.6672959327697754\n",
      "Train loss: 3.734086513519287\n",
      "Train loss: 4.125987529754639\n",
      "Train loss: 3.833317279815674\n",
      "Train loss: 3.773569345474243\n",
      "Train loss: 3.654771327972412\n",
      "Train loss: 4.1857733726501465\n",
      "Train loss: 3.6444091796875\n",
      "Train loss: 4.323836326599121\n",
      "Train loss: 4.25781774520874\n",
      "Train loss: 3.7654781341552734\n",
      "Train loss: 3.8378055095672607\n",
      "Train loss: 3.3077619075775146\n",
      "Train loss: 3.1887097358703613\n",
      "Train loss: 3.448354959487915\n",
      "Train loss: 3.365542411804199\n",
      "Train loss: 3.310636520385742\n",
      "Train loss: 3.9646575450897217\n",
      "Train loss: 3.304481267929077\n",
      "Train loss: 4.295783996582031\n",
      "Train loss: 4.258030891418457\n",
      "Train loss: 4.094170093536377\n",
      "Train loss: 4.495027542114258\n",
      "Train loss: 3.6806459426879883\n",
      "Train loss: 3.383538246154785\n",
      "Train loss: 4.318922996520996\n",
      "Train loss: 3.7638700008392334\n",
      "Train loss: 3.81355619430542\n",
      "Train loss: 3.762287139892578\n",
      "Train loss: 3.6386117935180664\n",
      "Train loss: 4.140359878540039\n",
      "Train loss: 3.593549966812134\n",
      "Train loss: 4.428979873657227\n",
      "Train loss: 4.186041831970215\n",
      "Train loss: 4.135149002075195\n",
      "Train loss: 3.713704824447632\n",
      "Train loss: 3.7412617206573486\n",
      "Train loss: 3.4059720039367676\n",
      "Train loss: 2.826415538787842\n",
      "Train loss: 3.5109386444091797\n",
      "Train loss: 3.910295009613037\n",
      "Train loss: 3.974236249923706\n",
      "Train loss: 4.278214931488037\n",
      "Train loss: 4.134350776672363\n",
      "Train loss: 3.521146059036255\n",
      "Train loss: 3.902153491973877\n",
      "Train loss: 4.000758171081543\n",
      "Train loss: 4.031332492828369\n",
      "Train loss: 3.561739444732666\n",
      "Train loss: 3.544200897216797\n",
      "Train loss: 4.0981125831604\n",
      "Train loss: 4.068879127502441\n",
      "Train loss: 4.072442054748535\n",
      "Train loss: 3.8465347290039062\n",
      "Train loss: 3.955288887023926\n",
      "Train loss: 3.624802350997925\n",
      "Train loss: 3.6122121810913086\n",
      "##################################################\n",
      "Epoch: 3\n",
      "Train loss: 4.252166271209717\n",
      "Train loss: 3.224911689758301\n",
      "Train loss: 3.810105323791504\n",
      "Train loss: 3.8098058700561523\n",
      "Train loss: 4.151799201965332\n",
      "Train loss: 4.2546539306640625\n",
      "Train loss: 3.685934543609619\n",
      "Train loss: 3.636137008666992\n",
      "Train loss: 3.708223342895508\n",
      "Train loss: 4.098659038543701\n",
      "Train loss: 3.7994542121887207\n",
      "Train loss: 3.7450668811798096\n",
      "Train loss: 3.633181095123291\n",
      "Train loss: 4.143499374389648\n",
      "Train loss: 3.6179416179656982\n",
      "Train loss: 4.299464225769043\n",
      "Train loss: 4.227550029754639\n",
      "Train loss: 3.7360386848449707\n",
      "Train loss: 3.810822010040283\n",
      "Train loss: 3.2806286811828613\n",
      "Train loss: 3.1164164543151855\n",
      "Train loss: 3.376941680908203\n",
      "Train loss: 3.3256449699401855\n",
      "Train loss: 3.288053274154663\n",
      "Train loss: 3.927687168121338\n",
      "Train loss: 3.261626720428467\n",
      "Train loss: 4.25408935546875\n",
      "Train loss: 4.228072166442871\n",
      "Train loss: 4.0607194900512695\n",
      "Train loss: 4.463382720947266\n",
      "Train loss: 3.6525962352752686\n",
      "Train loss: 3.3582661151885986\n",
      "Train loss: 4.295029640197754\n",
      "Train loss: 3.736495018005371\n",
      "Train loss: 3.7807135581970215\n",
      "Train loss: 3.7297768592834473\n",
      "Train loss: 3.6010706424713135\n",
      "Train loss: 4.100506782531738\n",
      "Train loss: 3.564744710922241\n",
      "Train loss: 4.398303031921387\n",
      "Train loss: 4.152413368225098\n",
      "Train loss: 4.114600658416748\n",
      "Train loss: 3.6642332077026367\n",
      "Train loss: 3.693413019180298\n",
      "Train loss: 3.3031563758850098\n",
      "Train loss: 2.759098529815674\n",
      "Train loss: 3.438013792037964\n",
      "Train loss: 3.8660967350006104\n",
      "Train loss: 3.9425559043884277\n",
      "Train loss: 4.246051788330078\n",
      "Train loss: 4.103084564208984\n",
      "Train loss: 3.490814208984375\n",
      "Train loss: 3.87388277053833\n",
      "Train loss: 3.9775047302246094\n",
      "Train loss: 4.001506805419922\n",
      "Train loss: 3.536585807800293\n",
      "Train loss: 3.5009078979492188\n",
      "Train loss: 4.076329708099365\n",
      "Train loss: 4.040389537811279\n",
      "Train loss: 4.048476219177246\n",
      "Train loss: 3.8219738006591797\n",
      "Train loss: 3.928992748260498\n",
      "Train loss: 3.60895037651062\n",
      "Train loss: 3.5705864429473877\n",
      "##################################################\n",
      "Epoch: 4\n",
      "Train loss: 4.227007865905762\n",
      "Train loss: 3.1798572540283203\n",
      "Train loss: 3.7813994884490967\n",
      "Train loss: 3.7666664123535156\n",
      "Train loss: 4.1235198974609375\n",
      "Train loss: 4.223860263824463\n",
      "Train loss: 3.6557412147521973\n",
      "Train loss: 3.605241537094116\n",
      "Train loss: 3.682546615600586\n",
      "Train loss: 4.0714616775512695\n",
      "Train loss: 3.7658915519714355\n",
      "Train loss: 3.7167749404907227\n",
      "Train loss: 3.6120336055755615\n",
      "Train loss: 4.101564407348633\n",
      "Train loss: 3.5916781425476074\n",
      "Train loss: 4.275330066680908\n",
      "Train loss: 4.197449207305908\n",
      "Train loss: 3.7068889141082764\n",
      "Train loss: 3.7841715812683105\n",
      "Train loss: 3.253829002380371\n",
      "Train loss: 3.0479068756103516\n",
      "Train loss: 3.308943748474121\n",
      "Train loss: 3.286384105682373\n",
      "Train loss: 3.265662670135498\n",
      "Train loss: 3.891185998916626\n",
      "Train loss: 3.2202517986297607\n",
      "Train loss: 4.212778091430664\n",
      "Train loss: 4.198637962341309\n",
      "Train loss: 4.027698516845703\n",
      "Train loss: 4.432085037231445\n",
      "Train loss: 3.6247668266296387\n",
      "Train loss: 3.333230495452881\n",
      "Train loss: 4.271246910095215\n",
      "Train loss: 3.7094950675964355\n",
      "Train loss: 3.748218536376953\n",
      "Train loss: 3.697753429412842\n",
      "Train loss: 3.566044330596924\n",
      "Train loss: 4.062614917755127\n",
      "Train loss: 3.5368475914001465\n",
      "Train loss: 4.367920875549316\n",
      "Train loss: 4.118858814239502\n",
      "Train loss: 4.094114303588867\n",
      "Train loss: 3.6177785396575928\n",
      "Train loss: 3.645948648452759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 1378.78it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 1326.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.205048084259033\n",
      "Train loss: 2.6956186294555664\n",
      "Train loss: 3.368621587753296\n",
      "Train loss: 3.822558879852295\n",
      "Train loss: 3.9110732078552246\n",
      "Train loss: 4.214263916015625\n",
      "Train loss: 4.072235584259033\n",
      "Train loss: 3.4612653255462646\n",
      "Train loss: 3.8458738327026367\n",
      "Train loss: 3.9547572135925293\n",
      "Train loss: 3.971832513809204\n",
      "Train loss: 3.5116024017333984\n",
      "Train loss: 3.459056854248047\n",
      "Train loss: 4.054853916168213\n",
      "Train loss: 4.012203216552734\n",
      "Train loss: 4.024623870849609\n",
      "Train loss: 3.7975587844848633\n",
      "Train loss: 3.9030022621154785\n",
      "Train loss: 3.593550205230713\n",
      "Train loss: 3.5295205116271973\n",
      "##################################################\n",
      "Epoch: 5\n",
      "Train loss: 4.202300548553467\n",
      "Train loss: 3.135711193084717\n",
      "Train loss: 3.7529470920562744\n",
      "Train loss: 3.7249362468719482\n",
      "Train loss: 4.09536075592041\n",
      "Train loss: 4.193469524383545\n",
      "Train loss: 3.626194477081299\n",
      "Train loss: 3.5746116638183594\n",
      "Train loss: 3.6570582389831543\n",
      "Train loss: 4.044397354125977\n",
      "Train loss: 3.732630968093872\n",
      "Train loss: 3.6886959075927734\n",
      "Train loss: 3.5913279056549072\n",
      "Train loss: 4.059976100921631\n",
      "Train loss: 3.565619707107544\n",
      "Train loss: 4.251436233520508\n",
      "Train loss: 4.16751766204834\n",
      "Train loss: 3.678027629852295\n",
      "Train loss: 3.7578561305999756\n",
      "Train loss: 3.2273647785186768\n",
      "Train loss: 2.9832563400268555\n",
      "Train loss: 3.244431972503662\n",
      "Train loss: 3.247775077819824\n",
      "Train loss: 3.2434661388397217\n",
      "Train loss: 3.85516619682312\n",
      "Train loss: 3.180333137512207\n",
      "Train loss: 4.1718573570251465\n",
      "Train loss: 4.1697258949279785\n",
      "Train loss: 3.995114326477051\n",
      "Train loss: 4.401136875152588\n",
      "Train loss: 3.5971593856811523\n",
      "Train loss: 3.308432102203369\n",
      "Train loss: 4.24757719039917\n",
      "Train loss: 3.6828722953796387\n",
      "Train loss: 3.7160820960998535\n",
      "Train loss: 3.6662278175354004\n",
      "Train loss: 3.533518075942993\n",
      "Train loss: 4.026660919189453\n",
      "Train loss: 3.5098559856414795\n",
      "Train loss: 4.337832927703857\n",
      "Train loss: 4.0853753089904785\n",
      "Train loss: 4.073678016662598\n",
      "Train loss: 3.5743753910064697\n",
      "Train loss: 3.598879337310791\n",
      "Train loss: 3.1118030548095703\n",
      "Train loss: 2.6360116004943848\n",
      "Train loss: 3.302807331085205\n",
      "Train loss: 3.779693603515625\n",
      "Train loss: 3.8797903060913086\n",
      "Train loss: 4.182852745056152\n",
      "Train loss: 4.041806221008301\n",
      "Train loss: 3.4324898719787598\n",
      "Train loss: 3.8181276321411133\n",
      "Train loss: 3.9325058460235596\n",
      "Train loss: 3.9423108100891113\n",
      "Train loss: 3.486787796020508\n",
      "Train loss: 3.4186220169067383\n",
      "Train loss: 4.033681869506836\n",
      "Train loss: 3.9843218326568604\n",
      "Train loss: 4.00088357925415\n",
      "Train loss: 3.7732901573181152\n",
      "Train loss: 3.877316951751709\n",
      "Train loss: 3.578589916229248\n",
      "Train loss: 3.4890260696411133\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "skipgram.train()\n",
    "skipgram = skipgram.to(device)\n",
    "optim = torch.optim.SGD(skipgram.parameters(), lr = learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(1, num_epochs+1):\n",
    "    print(\"#\" * 50)\n",
    "    print(f'Epoch: {e}')\n",
    "    for batch in tqdm(skipgram_loader):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device) # (B, W), (B)\n",
    "        output = skipgram(x) # (B, V)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        print(f'Train loss: {loss.item()}')\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-azerbaijan",
   "metadata": {},
   "source": [
    "## 테스트\n",
    "\n",
    "학습된 각 모델을 이용하여 test 단어들의 word embedding 을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "greenhouse-metadata",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 음식\n",
      "tensor([ 0.8414, -0.0927,  1.1790, -1.0684,  0.1599, -1.6787,  1.9308,  0.3619,\n",
      "         1.5416,  0.0210, -0.9105,  0.4918, -0.6280,  1.6087, -1.6134, -0.0077,\n",
      "         1.0283,  0.1557,  1.1997,  1.2250, -0.3184,  0.1810,  0.5651,  1.2415,\n",
      "        -0.5179,  1.3614, -0.4105,  0.3008,  0.2566,  1.5908,  0.2766,  0.4073,\n",
      "        -0.2811,  0.8772,  1.7411, -1.5562,  2.3794,  0.3100,  0.8502, -1.8186,\n",
      "         0.0955, -1.6632, -0.4454,  2.1830, -0.1331, -0.1485,  2.5633, -1.6486,\n",
      "        -0.9019, -0.0630,  2.0024,  2.4727, -0.7274, -1.0861, -0.4472, -1.0492,\n",
      "        -0.0955, -1.0498, -0.7079,  0.2181,  0.2404, -0.0777, -0.5834, -0.9832,\n",
      "        -0.2822,  0.6947, -1.3775, -0.1527, -0.3475,  1.9124, -0.5776,  1.5515,\n",
      "        -0.8664, -1.0768, -0.5148,  2.4771, -0.0880,  1.9284,  0.4005, -0.3472,\n",
      "        -0.5746, -0.5362, -0.9103, -1.3664, -0.9100, -0.6991, -0.8970,  0.4311,\n",
      "         0.0430, -0.7363, -0.2752, -1.8620,  0.5329, -2.3359, -2.2895,  0.4180,\n",
      "        -0.5917, -0.5738,  0.0756,  1.2278,  0.8576,  0.1157, -0.1303,  1.1794,\n",
      "         0.3988, -0.7520, -0.6414, -1.1228,  0.8460, -0.6923, -0.6921, -1.1084,\n",
      "         1.3991, -1.2487,  0.6911,  0.2146,  1.5819, -0.5411, -0.2448, -0.0226,\n",
      "         0.7969, -1.1806, -1.0712, -0.6676, -1.1945,  0.8075, -0.8780, -1.7552,\n",
      "         0.7837,  0.3134,  1.6758, -1.0211,  0.4537,  0.1665,  0.5574, -0.3797,\n",
      "         2.5688,  0.9923,  0.4666, -0.5243, -1.0404,  1.5429, -0.1796, -1.3203,\n",
      "         1.1945,  0.0998,  0.3906, -0.8219,  0.8074, -0.4135,  1.1243, -1.3822,\n",
      "         0.8022, -0.3517,  0.5828, -0.3980,  0.3242, -0.7720,  1.2054,  0.8929,\n",
      "         0.6519,  0.3298, -0.8350,  0.5942, -0.7308,  1.7700,  1.1147,  0.5520,\n",
      "        -0.0992,  0.7814, -1.2331, -1.9649,  1.3480, -1.0586,  0.5051,  0.2293,\n",
      "        -1.0606,  0.6503, -1.1773, -0.8325, -0.2599, -0.9592, -0.1562,  0.7857,\n",
      "         1.1497,  0.7778, -0.8583,  1.6743, -1.2279, -1.5313,  0.1480,  0.2077,\n",
      "        -1.2054, -0.8821, -0.8560, -0.9209,  0.5714, -0.8202,  0.3729,  1.3923,\n",
      "        -0.3635,  0.4269, -0.1466,  0.0294,  1.0152, -0.3453,  0.0109, -1.2521,\n",
      "        -1.0048,  1.0151, -1.3289, -0.1126,  0.2892, -0.3328, -1.0640, -1.0290,\n",
      "        -1.1168, -0.5334, -2.0852, -0.8245,  0.8506,  0.1715, -0.5184, -0.8150,\n",
      "         0.1669, -1.3193,  0.2177,  2.0165,  0.6882, -0.8683,  1.2378,  0.0542,\n",
      "         0.3976,  0.8043, -0.7298, -0.7643, -0.8705,  0.2081, -0.3513, -0.3524,\n",
      "        -0.2228, -0.4993, -0.3696, -1.1515,  1.2955,  1.3798, -0.8378,  0.1031,\n",
      "         0.7338,  0.5768,  0.6236, -1.1428,  0.5266, -0.2759,  0.5775,  0.5526],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Word: 맛\n",
      "tensor([ 0.1833,  2.5712, -0.4071, -1.0138, -0.6181, -0.6418, -1.0506, -1.3042,\n",
      "        -2.4809,  0.6684, -0.6033,  1.6550, -0.3273, -2.1457, -0.1795, -0.8705,\n",
      "         0.8730, -0.8048,  0.9350, -1.4464,  0.7440,  1.3229, -0.5982,  1.1037,\n",
      "         1.6316,  0.1523,  0.7377,  0.4411, -0.2434,  0.1588, -1.3376, -1.4402,\n",
      "        -0.2839, -0.6894,  0.1865, -0.6004,  0.3010, -0.0904,  0.4399,  0.5717,\n",
      "        -0.0288, -0.5268, -1.7013,  0.5866, -0.9624, -0.2158, -0.8797,  0.3162,\n",
      "        -0.3939,  1.4399,  0.6374, -1.0059, -0.1264, -0.1510, -1.4823, -0.2009,\n",
      "        -1.0218, -0.1530, -1.6312,  1.0818,  0.7030,  0.6203, -1.2280,  0.2003,\n",
      "         0.7440, -0.4452, -0.4557, -0.4407,  0.3114, -0.3272,  0.1141,  0.6477,\n",
      "         0.7816, -0.5613, -0.9635, -0.1394,  1.5689, -0.8610, -1.4998,  0.3608,\n",
      "        -0.6927,  0.9435, -0.0480,  0.7759, -0.5493, -0.5201, -0.6713,  0.4105,\n",
      "         0.4586, -0.7070, -0.6528, -1.2069, -0.3648,  0.1790,  0.5489, -2.5241,\n",
      "         0.8208,  0.5905, -1.7654, -0.8691, -1.2956, -0.5990, -1.3062,  1.3764,\n",
      "         1.0375,  0.7665,  0.2675, -0.4334, -0.5267,  1.7108,  1.3089, -0.5620,\n",
      "        -1.5701,  0.4660,  2.2534,  0.1453,  0.5896, -2.3843, -0.3315,  0.0603,\n",
      "         0.4591, -0.1947,  0.8791,  0.2436, -0.4172, -0.8727,  0.6324, -0.9706,\n",
      "        -0.7204, -0.3503,  0.8435, -0.5192, -0.1127,  0.0667,  1.6471,  0.4006,\n",
      "         0.7167, -1.2918,  0.3813, -0.6688, -0.0642,  0.3305, -1.0172, -1.3202,\n",
      "        -0.2141, -0.6396, -0.6535,  2.3214, -0.0602, -0.6126,  0.0729,  0.9802,\n",
      "        -0.3144, -0.0879,  0.5086,  1.3695, -1.2364,  1.7998, -1.0748, -1.0207,\n",
      "         0.4742,  0.5567, -1.3294, -0.8682,  0.0921,  0.0891, -0.7335,  0.8103,\n",
      "        -0.9027,  0.8277,  0.5223, -1.3866,  0.4540,  1.7203, -0.7497,  0.3938,\n",
      "         0.9144, -0.5706, -1.7492,  1.1131,  0.3396, -0.3419,  0.0976,  0.1552,\n",
      "         0.7110,  1.0466,  0.2791,  1.3434,  0.1164, -1.7434, -1.1089, -0.8355,\n",
      "         0.7490,  0.3040, -1.2847, -0.7981,  1.3110,  1.8564,  0.1704, -0.0825,\n",
      "        -0.4700, -0.1966,  0.4329,  0.3356, -1.4205, -0.1324, -0.7615,  0.4084,\n",
      "        -0.3553,  1.0276,  1.3550, -0.0345, -1.0637, -0.4159, -0.6237,  1.4495,\n",
      "         0.3230, -0.6697, -1.3513, -2.1136,  0.3530, -0.6380,  1.0161,  0.4165,\n",
      "         1.5134, -0.5812, -0.3442,  1.0885, -0.2455, -0.1466, -0.6839, -2.0945,\n",
      "         1.2190,  0.2888,  0.8974,  0.8818, -2.3394, -0.9109, -0.1798, -1.9851,\n",
      "         1.6735,  0.7471,  0.8309, -0.1401, -0.5316,  0.9627, -0.1909,  0.9548,\n",
      "         0.3620, -0.6681,  0.8814,  0.1727, -0.2933,  1.3133,  1.4968,  1.0029],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Word: 서비스\n",
      "tensor([-6.9261e-02,  8.8224e-01,  6.8612e-01,  5.8195e-02,  4.7143e-02,\n",
      "         1.1786e+00, -6.9114e-01,  1.7417e+00, -1.5121e+00,  1.0105e+00,\n",
      "        -9.7017e-01, -1.1540e-01,  1.1364e+00,  5.2625e-01,  2.7702e-01,\n",
      "         4.9712e-01, -7.4235e-01,  6.4504e-01, -1.1658e+00,  1.3659e+00,\n",
      "         2.8807e-01,  1.2520e+00,  6.5210e-01, -3.0884e-01,  6.0179e-01,\n",
      "         1.2756e+00,  8.0628e-01, -1.5606e+00, -1.5677e+00,  1.9381e+00,\n",
      "        -3.7656e-01, -3.3967e-01, -6.4261e-01, -1.3951e-01,  9.2349e-01,\n",
      "        -2.7627e-01,  7.9572e-01, -3.5478e-01,  5.4927e-01,  9.1155e-01,\n",
      "         2.4894e-01, -3.6870e-01, -6.6172e-01, -5.9726e-01,  2.6586e-01,\n",
      "        -1.3564e-02, -3.5330e+00, -3.4945e-01, -7.1134e-01,  1.4195e+00,\n",
      "         8.3309e-01,  1.0231e+00,  6.8510e-01, -3.1321e-01, -2.3471e-01,\n",
      "         3.1400e-01, -1.7430e+00,  6.1182e-01,  8.9766e-02, -5.1029e-01,\n",
      "        -1.0658e+00,  1.3550e+00,  8.5098e-01,  3.0757e-01, -3.9718e-01,\n",
      "        -3.1386e+00, -8.8309e-01, -1.1564e-01,  5.6222e-01, -4.4447e-01,\n",
      "        -1.4087e+00, -5.1352e-01,  1.6057e-03, -1.3005e-01, -8.8022e-01,\n",
      "         1.4682e-02,  8.5451e-02, -4.9703e-01, -1.6221e-01,  9.7293e-02,\n",
      "         8.4390e-01,  8.2197e-01,  3.8187e-01,  3.8381e-01, -3.7287e-01,\n",
      "        -3.6329e-01, -7.9499e-01, -6.8647e-01, -3.7165e-01, -1.8206e+00,\n",
      "        -2.6823e-02,  1.7406e+00, -6.5537e-01, -3.8221e-02,  1.0468e+00,\n",
      "         4.9884e-01,  8.8266e-01,  5.0663e-01, -3.7661e-01,  8.0551e-02,\n",
      "        -6.4635e-01, -1.4824e+00, -1.5807e+00, -7.9992e-01,  6.8758e-01,\n",
      "        -8.8826e-01,  4.3568e-01,  1.2789e-01, -1.1472e+00, -1.4456e+00,\n",
      "        -4.9406e-01,  1.4829e+00,  3.8060e-01,  7.7677e-02,  6.4303e-01,\n",
      "        -4.4759e-01, -5.3581e-01, -5.9730e-01,  7.5264e-01,  2.7077e-01,\n",
      "        -4.6304e-01,  1.8805e+00, -4.9697e-01,  1.2383e-01, -9.5209e-01,\n",
      "        -2.1242e-01,  1.8794e+00,  2.6937e-01,  7.6012e-01, -3.0628e-01,\n",
      "         1.9399e-01, -3.7808e-02, -1.3752e-01, -7.0040e-01,  1.2177e+00,\n",
      "        -5.0403e-01, -9.9622e-01, -1.2118e+00,  5.2059e-01, -1.9010e+00,\n",
      "         3.0504e-01, -8.4541e-01,  1.2413e+00, -8.6923e-02,  3.8582e-01,\n",
      "         8.4105e-01, -4.1907e-01, -5.2955e-01,  1.6955e-01,  1.3178e+00,\n",
      "        -1.5447e+00, -2.5194e-01, -8.0849e-01,  3.6145e-01, -1.3938e+00,\n",
      "         1.5601e+00,  1.3220e+00,  4.5912e-01,  1.6391e+00,  5.8881e-01,\n",
      "         4.2717e-01,  2.4510e-01,  1.1585e-01, -1.4503e+00,  1.4033e+00,\n",
      "         2.3028e+00,  1.3676e+00,  8.0571e-01, -1.5885e+00, -1.1777e+00,\n",
      "        -8.8305e-01,  2.5626e+00,  5.4627e-02, -1.2637e+00, -1.7357e+00,\n",
      "         5.1366e-01,  1.5336e+00, -1.0128e+00, -3.7883e-01, -1.8636e+00,\n",
      "        -2.1970e+00,  3.6666e-02,  7.1950e-01,  1.3943e+00, -2.3267e-01,\n",
      "         3.1174e-01, -1.1860e+00,  2.6264e-01, -1.0330e+00,  5.5809e-01,\n",
      "        -3.3902e-01, -1.3919e-01,  1.1600e+00,  1.0416e+00, -4.1862e-01,\n",
      "         8.0678e-01,  1.9224e+00,  1.2365e-01,  1.0106e-03,  1.3737e+00,\n",
      "         2.0461e-01,  6.0509e-01, -3.8565e-01,  1.1718e+00,  4.9413e-02,\n",
      "         8.5328e-01, -2.4614e+00,  8.9703e-02,  1.0863e+00, -1.6287e-01,\n",
      "         4.5755e-01, -6.2017e-01, -4.4532e-01, -9.1369e-01, -1.6998e+00,\n",
      "         1.0750e-01, -5.4877e-01,  8.3328e-01,  5.4449e-03, -1.3789e+00,\n",
      "        -2.6974e-01, -1.3619e+00, -4.1468e-01,  3.2899e-02,  1.0803e-01,\n",
      "        -5.2057e-01, -1.5524e+00,  1.2701e+00,  8.0176e-01, -5.0339e-03,\n",
      "         8.9263e-01, -4.8482e-01, -2.2524e-01, -2.9069e+00,  2.2092e-01,\n",
      "        -1.0388e+00,  8.7185e-01, -1.6006e-01,  4.9189e-01, -1.1079e+00,\n",
      "         1.0139e-01, -4.6394e-01,  4.0986e-01, -3.2294e-01, -1.8633e-01,\n",
      "        -5.5550e-01, -1.4079e+00, -1.8291e-01, -1.3419e+00, -4.0124e-02,\n",
      "        -5.6980e-02,  4.6909e-01,  4.5685e-01, -3.8926e-01, -5.7023e-01,\n",
      "         1.0565e+00], grad_fn=<SqueezeBackward1>)\n",
      "Word: 위생\n",
      "tensor([-3.5855e-01, -8.9199e-01,  1.5429e+00, -6.1688e-01,  1.8810e-01,\n",
      "         1.9275e-01, -3.9297e-02, -6.8426e-01, -2.5554e-01,  7.3732e-02,\n",
      "         2.2435e-01,  1.2368e+00,  9.0172e-01,  1.3559e+00,  1.6228e+00,\n",
      "        -9.9638e-02,  1.2563e+00,  7.7386e-02, -8.2070e-01,  2.1014e+00,\n",
      "        -7.3245e-02, -9.7237e-01, -4.0733e-01, -2.0267e-01, -2.4269e-01,\n",
      "         4.0307e-01, -7.5215e-01,  1.2709e+00, -4.5569e-01, -7.5987e-01,\n",
      "         7.2356e-01,  3.4087e-01,  6.5021e-01,  4.5302e-01,  8.7067e-01,\n",
      "        -2.3304e+00,  7.1575e-03,  1.8941e+00, -2.4760e-02,  7.8075e-01,\n",
      "        -7.1184e-01,  3.8375e-01,  8.1250e-01,  1.0103e+00, -9.0887e-01,\n",
      "         7.9145e-01, -8.2542e-01,  2.9530e+00, -3.6206e-01,  5.2297e-01,\n",
      "         1.5720e+00,  3.9805e-01,  1.4716e+00,  8.0245e-01, -5.1944e-01,\n",
      "        -1.7343e+00,  2.7814e-01,  1.0952e+00,  1.3078e-01,  5.7772e-01,\n",
      "         8.9244e-01, -4.8972e-01, -1.4694e-01, -1.2950e-01, -2.2752e+00,\n",
      "        -2.5162e+00,  7.6564e-01,  3.1423e-01,  6.0300e-02, -1.8022e+00,\n",
      "        -8.8005e-01,  9.1228e-03, -2.0645e+00, -7.3736e-01, -3.8648e-01,\n",
      "        -5.4303e-01, -8.0173e-01,  5.5901e-01,  6.1147e-01,  1.8951e+00,\n",
      "         5.7373e-01, -1.8282e+00, -1.0294e+00, -5.3837e-01, -4.5405e-01,\n",
      "        -1.8450e-01,  9.4204e-02,  1.8534e+00, -3.4474e-01,  4.3965e-01,\n",
      "        -1.0118e+00,  1.2668e+00,  1.7356e+00, -7.4625e-01,  5.6776e-01,\n",
      "        -5.6953e-01, -9.0647e-01, -3.3836e-01,  2.7634e-02, -6.0816e-02,\n",
      "         1.2705e+00, -2.6537e-01, -2.4097e-01, -3.8735e-01,  4.8733e-01,\n",
      "        -1.8692e-01, -4.6801e-01,  1.5957e-01,  9.8959e-02,  1.4793e+00,\n",
      "        -8.4646e-02,  5.3315e-01,  7.0754e-01, -8.5061e-01,  7.8901e-01,\n",
      "        -7.2229e-01,  1.0920e-01, -1.1015e+00,  9.9618e-02,  5.8825e-01,\n",
      "         7.6034e-01,  6.0446e-01,  1.2815e-03, -2.0970e+00,  2.4382e-01,\n",
      "         8.1602e-01,  4.6519e-01,  7.4404e-01,  1.2494e-01, -1.4831e+00,\n",
      "        -1.3630e+00,  9.0778e-01, -6.8916e-01,  9.8377e-01,  4.2316e-01,\n",
      "        -9.2111e-02,  2.0942e+00, -4.9804e-01, -5.9203e-02,  1.4530e+00,\n",
      "         1.1326e+00, -5.8368e-01,  2.9278e+00, -6.7160e-01,  1.6470e-01,\n",
      "        -1.2711e+00,  1.0588e+00, -8.4619e-01,  1.7051e+00,  1.0005e+00,\n",
      "         6.7606e-01, -7.0558e-01, -2.9672e-01, -3.5315e-01,  2.3636e+00,\n",
      "         3.0257e-01, -1.6996e+00,  4.5847e-02,  5.7933e-01,  7.7812e-01,\n",
      "        -1.1376e+00, -2.2173e-01,  2.4826e-02,  6.3487e-01,  2.0094e-01,\n",
      "        -1.0865e+00, -7.6639e-01,  8.2514e-01, -1.0971e+00,  9.1602e-01,\n",
      "        -1.7745e+00,  1.1020e+00, -6.7951e-02, -7.9540e-01, -6.7937e-01,\n",
      "         9.8706e-01,  1.3585e+00,  9.3878e-01,  7.8884e-02, -1.1357e+00,\n",
      "         3.5380e-01,  3.5272e-01,  9.8871e-01,  8.6851e-01,  4.7180e-01,\n",
      "        -2.0901e-01, -3.9180e-01,  1.6216e+00,  1.3743e+00,  5.2001e-02,\n",
      "        -6.5250e-02, -1.2194e+00, -1.0225e-01,  1.1994e+00,  8.5906e-01,\n",
      "        -8.2648e-02, -6.3604e-01, -2.7737e-02,  3.4972e-01,  1.0293e-01,\n",
      "         5.7845e-01, -7.6904e-01, -2.1721e-01,  6.1208e-01,  1.1338e+00,\n",
      "         6.8285e-01, -7.3229e-01,  3.0330e-01,  1.1804e+00, -6.6736e-01,\n",
      "         7.1591e-01,  1.5369e+00, -1.1133e+00,  1.1240e+00,  1.3534e+00,\n",
      "         4.9522e-01, -5.2171e-01,  1.1142e+00,  1.2441e+00,  2.0272e-01,\n",
      "         1.3807e+00, -2.1033e-01,  3.0320e-01,  6.3461e-01,  9.0004e-01,\n",
      "        -5.1593e-01,  9.0229e-01, -3.8616e-01, -7.5566e-01,  1.5944e+00,\n",
      "         3.1815e-02,  1.1841e+00,  6.7135e-01, -3.7372e-01,  1.8582e-01,\n",
      "        -9.9385e-01,  1.0978e+00, -1.2421e+00, -3.3196e-01, -8.9056e-01,\n",
      "        -6.9340e-01, -2.0233e+00, -2.0406e-02, -1.4000e+00, -1.1407e+00,\n",
      "         1.0087e+00,  2.1921e+00, -6.1334e-01, -4.4275e-01,  1.0998e+00,\n",
      "         3.3186e-01, -1.6954e+00, -1.4835e-01, -6.6505e-01, -2.9609e-01,\n",
      "        -1.3727e+00], grad_fn=<SqueezeBackward1>)\n",
      "Word: 가격\n",
      "tensor([-5.6643e-01, -2.1940e+00, -2.3425e-02, -1.8259e-01,  1.7763e-01,\n",
      "        -4.3964e-01,  5.4205e-02, -6.9891e-01, -3.8914e-01, -1.9664e+00,\n",
      "         1.0252e+00,  5.8356e-01,  3.7951e-01,  2.3949e+00,  8.9961e-01,\n",
      "        -8.7890e-01, -3.8083e-01,  9.0754e-01,  6.6191e-01, -2.2918e-01,\n",
      "         5.7329e-01, -1.0874e+00,  4.2628e-01,  2.6669e-01, -2.7445e-01,\n",
      "        -3.9689e-01, -1.4825e+00, -6.1399e-01, -6.0653e-01, -2.1174e+00,\n",
      "        -9.1358e-01,  9.6686e-01,  1.4013e-01,  1.5852e-02,  1.1353e+00,\n",
      "         1.4798e-01,  9.2618e-01,  6.9766e-01,  1.4245e+00,  7.1970e-01,\n",
      "         1.9524e+00, -1.5178e+00, -4.7489e-01, -2.2807e-01,  5.6573e-01,\n",
      "        -1.1632e+00,  1.4441e+00, -6.4854e-01, -1.3410e+00,  1.7747e-01,\n",
      "        -1.4319e-01, -8.1074e-01,  1.7370e-01, -5.4981e-01,  5.2465e-01,\n",
      "         3.7774e-01,  2.9870e-01, -2.7937e+00,  1.4209e-01,  3.0854e-01,\n",
      "         1.5686e+00,  1.0770e+00, -8.1843e-01, -4.0228e-01, -1.1806e+00,\n",
      "        -1.2948e+00, -7.6836e-01,  3.2814e-01,  1.2415e+00, -3.4849e-02,\n",
      "        -2.3091e-01,  1.4081e+00,  2.1777e+00,  1.7419e-01, -9.8392e-01,\n",
      "        -1.2205e+00,  1.0920e-01, -8.5756e-01,  1.1487e+00,  1.2442e+00,\n",
      "         5.5296e-01, -8.7900e-01,  4.0176e-01,  1.0254e-01,  1.2698e+00,\n",
      "         4.5200e-01,  1.3078e+00, -5.3452e-01,  8.5434e-01,  8.1891e-01,\n",
      "        -1.0752e+00, -2.0218e+00,  6.6442e-01, -1.2915e+00,  3.8542e-01,\n",
      "        -9.6499e-01,  4.0598e-01,  6.0342e-01, -6.7665e-01, -3.1458e-01,\n",
      "         5.8146e-01,  1.9041e-01,  1.7023e-01, -6.8019e-01, -9.2812e-02,\n",
      "         6.6635e-01,  1.7127e+00,  2.1380e+00,  1.0015e+00,  3.1037e-02,\n",
      "        -9.8084e-01,  1.0507e+00,  8.2505e-01, -1.5464e+00, -2.4015e-01,\n",
      "         4.1999e-02,  8.4815e-01, -1.5362e-01, -7.5143e-01, -6.3864e-01,\n",
      "         1.9550e-01,  5.4878e-01, -1.8508e+00,  1.4950e-02,  9.8557e-01,\n",
      "         1.3937e-01, -1.8796e+00, -4.5353e-01,  1.1802e+00, -1.1520e+00,\n",
      "         9.5274e-01, -2.5690e+00,  9.0619e-01,  1.3724e-01, -5.6797e-01,\n",
      "         2.1183e-02,  2.8464e-02,  3.1599e-03,  1.8782e-01, -3.6102e-02,\n",
      "        -7.0622e-01, -1.0773e+00, -3.7057e-01, -1.4760e+00, -3.2168e+00,\n",
      "         1.0807e+00,  2.7312e+00, -7.9705e-02, -1.9842e+00,  2.3207e+00,\n",
      "         8.9925e-01, -8.5941e-01,  1.7765e+00, -1.7640e+00,  3.3891e-01,\n",
      "         1.0125e+00, -7.3480e-01, -1.0144e+00,  5.7944e-01,  2.2357e+00,\n",
      "         1.2684e+00, -6.8977e-01,  4.7535e-01,  5.3468e-01,  3.1959e-01,\n",
      "         1.1675e+00, -8.1425e-01,  1.1876e+00,  1.0987e+00,  5.2440e-01,\n",
      "        -3.9515e-01,  4.7309e-01,  9.5700e-01,  7.9400e-01,  9.8200e-01,\n",
      "         6.8948e-01, -1.2788e+00, -8.2186e-01, -4.9225e-01,  7.6871e-01,\n",
      "        -6.9040e-01,  7.8449e-01, -1.0994e+00,  1.0239e+00,  5.5732e-01,\n",
      "        -5.9111e-01, -8.0094e-01,  1.3330e+00,  2.4908e+00, -2.5998e-01,\n",
      "        -6.4920e-01, -2.6610e-01,  5.8880e-01,  6.1464e-01,  1.0772e+00,\n",
      "        -9.4381e-01, -1.9275e-01, -8.9705e-01, -1.1733e+00,  1.2913e+00,\n",
      "        -5.5057e-01,  1.6619e-01, -7.2764e-01, -7.8469e-01, -9.8113e-01,\n",
      "        -9.3262e-02,  1.1316e+00,  2.5297e-01,  9.1303e-01, -5.3839e-01,\n",
      "         6.6714e-01, -1.0512e+00, -1.2214e+00,  5.2987e-02, -3.5946e-01,\n",
      "         1.5925e-01, -7.7821e-01,  1.4316e+00, -4.1405e-01,  1.6502e+00,\n",
      "        -7.2455e-01, -9.0311e-02,  1.2898e+00,  3.1654e-02, -2.0618e-01,\n",
      "         2.0791e-01,  2.3305e+00, -7.0847e-01, -6.1487e-01,  4.0744e-01,\n",
      "        -1.7739e-02,  2.1457e-01,  7.0590e-01,  5.3948e-01, -9.4669e-01,\n",
      "         2.4568e-01,  9.7024e-01, -3.0914e-01, -1.8787e+00,  4.7827e-01,\n",
      "        -1.6165e+00, -1.2314e+00, -1.0910e+00, -9.4693e-01, -1.1075e+00,\n",
      "        -6.9341e-02, -8.0349e-01,  8.6473e-01, -4.5115e-01, -2.9354e-01,\n",
      "         1.1191e+00,  5.3747e-01, -1.0313e+00,  4.7765e-01, -4.3378e-01,\n",
      "         1.3840e-01], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
    "    emb = cbow.embedding(input_id)\n",
    "    \n",
    "    print(f'Word: {word}')\n",
    "    print(emb.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "composed-drilling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7312, grad_fn=<UnbindBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(max(emb.squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "powerful-collectible",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 음식\n",
      "tensor([-2.1371e+00,  2.8544e+00, -1.0220e+00,  2.8298e+00, -8.0401e-01,\n",
      "         1.1058e-01,  1.5712e+00,  9.4139e-01,  1.0837e+00, -3.1570e-01,\n",
      "         4.9930e-01, -1.1790e+00,  9.7276e-01, -1.1045e+00, -6.5034e-01,\n",
      "        -7.3888e-01,  6.2121e-01, -5.2914e-02, -1.9994e+00,  3.7437e-01,\n",
      "        -4.0234e-01, -8.7200e-01,  9.3793e-01,  4.9113e-01,  4.1820e-01,\n",
      "        -8.8620e-01,  1.1313e+00, -5.6042e-01, -1.0576e+00, -2.0703e+00,\n",
      "         1.0570e+00,  3.2552e-01,  7.7578e-01,  1.3018e+00,  8.8480e-01,\n",
      "        -1.3476e+00, -1.3771e+00, -1.8304e-01, -2.2765e-01,  8.9935e-01,\n",
      "         1.3754e-01, -4.1829e-01,  1.2732e+00,  1.1498e+00, -3.4014e-01,\n",
      "         5.3145e-01,  1.2951e+00, -4.2927e-01, -1.2027e+00,  3.2702e-01,\n",
      "        -1.1742e+00,  1.6044e+00, -9.9670e-02,  1.4488e-01,  1.6443e-01,\n",
      "         4.7610e-01, -1.3051e+00, -2.4355e+00,  7.9859e-01,  2.8453e-02,\n",
      "         5.5095e-01,  1.1815e+00,  1.7551e-01, -1.9217e-01, -8.6884e-01,\n",
      "         2.6721e-01, -6.0254e-01,  8.8265e-01, -8.2328e-01, -1.9127e-01,\n",
      "         4.0192e-01,  3.3019e+00, -9.2608e-01, -2.7812e-01, -4.9109e-01,\n",
      "        -4.8479e-01, -7.7563e-01,  2.0143e-01,  2.1458e-01, -1.1560e+00,\n",
      "        -5.6869e-01,  8.4987e-01,  9.8765e-01,  1.2445e+00,  1.4650e+00,\n",
      "        -4.3448e-01, -3.7498e-01,  1.4434e-01, -5.1619e-01, -4.8611e-02,\n",
      "         1.3684e+00, -3.2872e-01, -4.8412e-01, -1.5027e+00, -3.1742e-01,\n",
      "         4.5609e-02, -5.6923e-02,  1.7263e+00, -7.0898e-01,  4.9507e-01,\n",
      "        -2.6992e-01, -1.4037e+00,  1.1974e+00, -9.1028e-01, -7.5981e-01,\n",
      "         1.0770e+00,  1.6461e-01,  6.2534e-01, -1.6694e+00, -3.7355e-01,\n",
      "        -6.7863e-01, -7.4943e-01, -1.1030e+00, -1.7645e+00,  1.2337e-04,\n",
      "        -1.2896e+00, -9.9131e-02,  6.4868e-01,  5.1330e-01, -9.1133e-01,\n",
      "        -3.0837e-01, -5.9318e-01, -1.2293e+00,  2.2401e+00,  9.9663e-02,\n",
      "        -7.1188e-01, -1.2607e+00,  3.0473e-01,  2.3832e-01, -1.5559e-02,\n",
      "        -6.7268e-01,  6.6890e-01,  1.1704e+00,  1.9726e+00, -1.3301e+00,\n",
      "         3.5331e-01,  3.9483e-01, -1.8362e-01, -4.8621e-01, -3.6944e-01,\n",
      "        -8.3167e-01,  3.6770e-01, -7.0644e-01,  4.2300e-01, -3.6432e-01,\n",
      "        -4.4655e-01, -5.1179e-01, -5.0969e-01, -4.8600e-01, -1.6169e-01,\n",
      "        -1.1882e+00, -9.6719e-01,  8.6774e-01,  2.1210e+00, -3.4184e-01,\n",
      "         5.1731e-01, -1.1637e+00,  8.8371e-01,  4.6877e-01, -2.2410e+00,\n",
      "         8.4504e-01,  1.5369e+00, -1.0816e+00, -9.8893e-01, -1.0974e+00,\n",
      "         2.5129e-02,  6.3731e-01, -4.3357e-01,  5.2755e-01,  1.3856e-01,\n",
      "         9.3774e-01,  1.8326e+00,  9.6605e-01, -7.6291e-01,  1.1253e+00,\n",
      "         4.5373e-01,  2.4458e-01, -1.5732e-01,  6.5820e-01, -8.8461e-01,\n",
      "         1.4438e+00, -4.5575e-01,  2.3705e-01, -5.5647e-01, -1.4033e+00,\n",
      "         2.9502e+00,  4.9494e-01,  5.7102e-01, -2.5856e-01,  8.3580e-01,\n",
      "        -1.4232e+00,  4.0364e-01, -4.2990e-01,  1.1022e+00, -1.1139e-01,\n",
      "        -7.1144e-01, -2.5023e-01,  1.1444e+00, -1.7381e+00,  1.1096e+00,\n",
      "        -9.7848e-01,  1.0818e+00,  1.0751e+00, -5.5807e-02, -1.7743e+00,\n",
      "        -6.5290e-01, -6.7235e-01, -8.5965e-01, -3.5026e-01,  1.3025e+00,\n",
      "         1.1995e+00,  7.7247e-01,  2.3936e-02, -1.6795e+00, -3.0619e-03,\n",
      "        -3.1711e-01,  1.1007e+00, -3.8407e-01, -6.4005e-01,  2.5634e-01,\n",
      "         1.6179e-01, -1.8615e-01,  4.5353e-01, -2.5508e+00,  1.5954e+00,\n",
      "        -3.4342e-01,  4.5733e-01,  1.4737e+00,  6.2799e-01,  9.6505e-01,\n",
      "         7.8796e-02, -8.1651e-01, -1.2002e-01, -1.3970e+00,  7.9777e-01,\n",
      "         1.4767e+00,  6.9683e-01,  1.4840e+00, -3.4146e-01, -3.3030e-01,\n",
      "        -1.4329e+00,  7.4383e-01,  8.1702e-01, -2.5440e+00, -4.3249e-01,\n",
      "         1.8802e+00, -6.0931e-01, -6.3269e-01, -1.0196e-01,  1.1028e-01,\n",
      "         9.4950e-01,  2.6993e-01, -2.5989e-01,  6.2489e-01, -2.4757e-03,\n",
      "         6.3783e-01], grad_fn=<SqueezeBackward1>)\n",
      "Word: 맛\n",
      "tensor([-0.0960, -0.0548,  0.2690,  0.7512, -1.5652, -1.0954, -0.1980, -0.7221,\n",
      "         1.8518,  0.8467,  0.6469,  1.4747,  0.2210,  0.2221, -1.3035, -0.4701,\n",
      "         2.1609, -0.7931, -1.7450,  0.3078,  1.4725,  0.8422, -0.3600,  0.9891,\n",
      "        -0.2631,  0.1267,  0.3276,  0.9999,  1.3857,  1.9201,  1.7491,  1.3244,\n",
      "         0.9148, -1.3809,  1.6711, -0.0042, -1.3761, -0.8752,  0.6232,  1.1372,\n",
      "         0.9573,  1.1239,  1.3911, -0.9399, -0.5898, -1.0535, -1.8572,  0.4406,\n",
      "        -0.1674,  2.2219, -0.3170,  0.6210,  1.5321, -0.4447,  1.0568, -0.7371,\n",
      "        -1.5945,  0.0301,  0.1737,  1.0739,  0.3500,  0.3143, -2.0160, -0.3682,\n",
      "         0.3450,  2.1708,  1.3685,  0.2239,  1.2524, -0.7693, -0.0560,  1.7072,\n",
      "        -0.6708, -2.0144,  1.5170, -1.2227, -0.2323,  0.0645,  2.3366,  0.7347,\n",
      "         0.7826, -0.1242,  0.5845,  0.1151,  0.8255, -0.8667,  1.9499,  0.2370,\n",
      "        -0.0644, -0.3916,  0.8029, -1.7146,  0.5184, -0.0066,  0.2691, -0.7811,\n",
      "        -0.8838, -0.3190,  0.4139, -0.2033,  1.7856,  0.5565,  2.8187,  1.7688,\n",
      "        -1.2519, -0.4175,  0.0077,  1.4960,  0.3691,  0.0417,  1.1304, -0.1955,\n",
      "         0.4650,  0.1489,  0.5076, -0.4915, -0.6203,  0.8316,  0.8599,  0.7621,\n",
      "         1.2382,  0.1538, -0.4743, -0.1931, -1.3594, -0.3327, -0.6354, -0.2388,\n",
      "        -0.4726, -0.5838,  0.7209,  1.1988,  1.3421, -1.4338, -0.5169, -0.6109,\n",
      "        -1.5237, -0.3268, -0.5306, -0.9025, -1.3431,  1.3358,  1.2369,  0.9543,\n",
      "        -0.5199,  0.5176,  1.5053, -0.4834, -1.1322,  0.8557,  0.8836, -0.5872,\n",
      "        -0.1249, -0.6986,  1.0830,  1.3536,  1.0027, -0.5910,  1.9414, -0.1024,\n",
      "         1.5069, -0.0099,  1.0437,  0.6127, -0.1096,  1.8538, -1.4560, -0.7906,\n",
      "        -2.7140, -0.7432, -0.9547,  1.2347,  1.0478, -0.8063, -0.4097, -0.9090,\n",
      "        -1.9047,  1.2009, -1.1808,  0.4192,  0.5184, -0.5244,  0.7781, -0.5029,\n",
      "        -0.4160,  0.2503,  1.3248, -0.0974, -0.8025,  0.3086, -0.0614,  0.9197,\n",
      "        -0.1738, -2.0849, -0.3975,  2.2456,  0.1680,  0.3207,  0.9372,  2.4668,\n",
      "         0.7743, -1.6244,  0.4203, -0.4721,  0.0363, -0.4329, -0.1615,  0.9985,\n",
      "         2.5204, -0.6941,  2.0825,  0.0587, -0.2536,  2.1479, -1.4245, -0.7586,\n",
      "         0.6471, -0.0827,  0.5693,  0.4707,  0.8720, -0.3179,  0.4878,  1.5403,\n",
      "        -0.7738, -1.2009,  0.4878, -1.8336,  2.0115, -0.6474, -0.5506, -0.3373,\n",
      "        -0.5207,  0.2485,  0.3314,  3.2477, -0.3840,  1.6348, -0.3517, -0.6153,\n",
      "        -0.0990,  0.6162, -0.1936, -2.3995,  0.0676,  0.3445, -0.8988,  0.0709,\n",
      "         0.5492,  0.3320, -0.5891, -1.1386, -0.1884,  1.0360, -1.5669,  1.2196],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Word: 서비스\n",
      "tensor([-7.8419e-02, -1.1141e+00, -3.2267e-02, -1.0244e+00, -1.1217e+00,\n",
      "        -1.2234e+00,  3.1631e-02,  2.1154e-01,  8.0336e-01, -1.1493e+00,\n",
      "         9.7571e-01,  1.2878e+00, -8.5879e-01, -4.2758e-01,  5.9222e-02,\n",
      "         3.6411e-01, -1.6241e-01, -3.5290e-01, -7.0540e-01,  5.8144e-01,\n",
      "        -2.6459e-01, -5.8544e-01, -6.0836e-01,  3.3598e-01,  4.9663e-01,\n",
      "         1.0131e+00,  8.5717e-01,  1.8602e-02,  1.1200e+00,  8.1364e-01,\n",
      "        -9.7593e-01, -1.4590e+00,  1.1378e+00, -1.0477e+00,  1.1931e+00,\n",
      "        -7.2314e-01,  5.2290e-01,  6.5489e-01,  7.0189e-01, -6.2387e-01,\n",
      "        -1.5836e+00, -5.5279e-01,  3.6862e-01, -1.0636e+00,  6.9436e-01,\n",
      "        -1.0031e+00, -9.2279e-01, -5.0747e-01,  6.0618e-01,  4.5179e-01,\n",
      "        -5.4972e-04, -8.8346e-01, -2.7094e-01, -2.6897e+00, -3.0418e-01,\n",
      "        -2.3724e-01, -2.0731e+00,  8.4065e-01, -1.0748e+00,  7.6192e-01,\n",
      "         2.4255e+00, -3.6312e-01,  1.4001e-01,  5.4235e-01,  9.4090e-01,\n",
      "        -9.9889e-01,  9.6383e-01, -3.0524e-01,  1.0735e-01, -1.2800e+00,\n",
      "         2.5461e-01, -1.0539e+00, -5.0722e-01, -1.4072e+00,  4.8055e-02,\n",
      "         1.8594e-01, -4.5557e-01, -6.6782e-01, -2.0034e-01, -2.0378e+00,\n",
      "         1.9467e-02,  4.8671e-01,  2.4435e-01,  4.6914e-01,  6.0823e-01,\n",
      "        -7.8967e-01,  3.6170e-01, -9.1648e-02,  1.1933e+00,  5.5598e-01,\n",
      "        -7.1942e-01,  9.8357e-01, -1.4368e+00,  3.7804e-01,  1.8043e+00,\n",
      "        -7.1486e-01, -4.0184e-01,  9.1316e-01, -5.9262e-01,  5.0392e-02,\n",
      "        -2.1362e+00,  4.9757e-01,  2.3498e+00, -6.9856e-01, -2.5806e-01,\n",
      "        -1.8490e+00, -3.2815e-01,  9.2957e-01, -8.7773e-01,  2.8991e-01,\n",
      "         6.8063e-01, -2.6908e-01, -3.0019e-02, -3.7216e-02,  4.4863e-02,\n",
      "        -1.0568e+00,  2.3280e+00, -1.1224e+00, -4.8192e-01,  1.2146e+00,\n",
      "         1.7315e+00,  3.5162e-01, -5.8341e-01, -3.1500e-01, -9.7690e-01,\n",
      "         3.3502e+00,  1.2372e+00, -2.0291e+00,  3.2318e-01, -5.1951e-01,\n",
      "         1.9143e+00, -7.4491e-01,  7.2728e-02, -7.4490e-01, -8.8284e-01,\n",
      "        -6.9573e-01,  7.4595e-01,  8.7163e-02, -3.4356e-01, -6.9609e-01,\n",
      "         2.1104e-01, -6.6635e-01, -9.0299e-01, -2.3096e-01,  7.6346e-01,\n",
      "         3.4606e-01, -3.4382e-01, -6.3136e-02, -1.2071e-01, -5.0262e-01,\n",
      "        -5.5328e-01,  3.0539e+00, -6.2961e-01, -9.0028e-01,  1.0028e+00,\n",
      "         1.2802e+00,  6.2941e-01,  9.1419e-01,  1.3542e+00, -3.2907e-01,\n",
      "         5.9550e-01,  1.8443e+00,  7.0103e-01, -2.9213e-04,  8.7410e-01,\n",
      "         4.6860e-01, -1.2974e+00, -6.4896e-01, -6.6553e-01, -1.5094e+00,\n",
      "         1.0865e+00, -1.1539e+00, -3.3869e-01, -5.9137e-01, -1.2017e+00,\n",
      "         5.3327e-01, -1.2914e+00,  9.4771e-01,  1.3409e+00, -8.8160e-01,\n",
      "        -1.3658e+00, -8.4440e-02,  1.1111e+00,  7.9306e-01,  1.0240e+00,\n",
      "        -1.4586e+00,  6.5475e-01,  1.6611e+00,  5.7159e-01, -4.1148e-01,\n",
      "         7.4438e-01,  1.7756e+00, -7.3332e-01,  3.2498e-01, -9.6286e-01,\n",
      "         6.1420e-01, -8.6208e-01, -6.9953e-01, -1.5038e+00,  8.9539e-01,\n",
      "         6.6315e-01, -1.5708e+00,  6.7980e-01,  1.0116e+00,  2.6147e-01,\n",
      "        -1.3079e-01, -5.5290e-01, -8.4449e-01,  2.1203e-01,  6.6321e-01,\n",
      "        -6.7590e-01, -3.6269e-01,  3.8865e-01,  5.5069e-01, -1.6559e+00,\n",
      "        -5.2629e-01,  4.0973e-01,  7.7381e-01, -7.3248e-01, -3.9673e-01,\n",
      "         3.3854e-01, -9.8002e-03, -3.7440e-01, -8.1504e-01,  9.9052e-02,\n",
      "        -8.5813e-01,  3.5481e-02,  1.9520e+00,  5.5229e-01,  1.0201e+00,\n",
      "         1.7120e+00,  6.0164e-02,  5.2788e-01, -6.7140e-02, -1.9175e-01,\n",
      "        -9.1673e-01, -1.4229e-01, -1.6638e+00,  1.2517e+00,  1.2238e-01,\n",
      "         3.5941e-01, -1.0213e-01, -1.9323e+00, -3.0095e-01, -1.0929e+00,\n",
      "        -1.9969e-01,  2.1185e-01, -5.4301e-01,  7.4306e-01, -2.3608e+00,\n",
      "        -1.1114e+00, -1.5930e+00,  2.3841e-01, -1.1290e-01, -6.3415e-02,\n",
      "         8.0040e-01], grad_fn=<SqueezeBackward1>)\n",
      "Word: 위생\n",
      "tensor([-1.2203, -1.5602, -0.3510, -0.1819, -1.3189, -0.6363,  0.8762, -0.8612,\n",
      "         2.1134,  0.1648, -1.7719, -1.2814, -0.6242, -2.6049, -0.3731,  0.4360,\n",
      "         0.3917, -0.6383,  0.2657,  1.2900, -1.8465, -0.8355,  0.6392,  0.8099,\n",
      "        -0.2211, -0.0690, -0.8458, -1.7203, -1.1108,  0.0444,  0.3718,  0.4588,\n",
      "        -1.3624, -0.9479, -0.3398,  1.8029,  0.3531, -1.7647, -0.6866,  0.7102,\n",
      "        -0.1668,  1.3460,  0.3986, -1.0928, -1.6330,  0.4379, -0.0073, -0.1000,\n",
      "         0.3018, -1.4654,  0.7858,  0.9674,  0.6351, -1.4595,  1.8375,  2.1341,\n",
      "        -0.4962,  0.4066,  0.3456, -1.4388,  2.4422,  0.6777,  0.7931,  0.5771,\n",
      "        -0.8477, -0.8532, -0.4966, -0.7848, -0.5178, -1.3943, -0.0624,  0.1329,\n",
      "        -0.2574,  0.6315,  0.6023,  0.3938,  1.0958,  0.2524,  1.0593, -0.1012,\n",
      "        -1.1692, -0.0693, -0.5704, -0.4642,  0.6584, -1.6939, -0.0287, -2.0225,\n",
      "         0.4039, -0.1301,  0.0115, -1.4434,  0.6335,  0.3204,  0.2218,  0.3935,\n",
      "         1.8558, -0.0893, -0.6268, -1.2894,  1.2200,  0.7308,  0.8861, -0.1658,\n",
      "        -0.5438,  0.0142, -0.7240, -2.0757, -0.5012, -0.6104, -0.2523,  0.3329,\n",
      "        -2.8851,  1.4842,  0.1489, -0.0969, -1.2506,  0.3625,  1.7251,  0.6079,\n",
      "         0.5691,  0.3330,  0.4656,  0.4744, -0.5851,  2.2502, -0.1033, -1.3430,\n",
      "        -1.2450,  0.9367, -0.8600,  0.1541, -0.2457,  1.5711,  1.7432,  0.0342,\n",
      "        -0.4780, -0.2056,  1.2149,  1.6821, -0.6138,  0.3181, -1.2257, -1.2724,\n",
      "        -1.5348, -0.0074,  2.0350,  0.5022,  1.3156,  0.0423, -0.2817,  0.0536,\n",
      "         0.3054,  0.2443,  0.4002,  0.9812, -0.0856, -0.8681,  0.6374, -0.9637,\n",
      "        -1.2232,  0.1531,  2.4679, -1.1629, -0.8922,  1.9644, -0.3582, -0.6999,\n",
      "         0.1698,  1.4544,  0.1757, -1.8042,  0.4620,  1.7043, -0.6121, -0.2287,\n",
      "        -0.7029, -0.2422,  0.3648, -0.3323, -3.0975,  1.1277, -1.0752,  0.7785,\n",
      "         1.3628, -0.3683, -0.7277, -0.0772,  0.9279,  1.1066,  1.2071,  1.5986,\n",
      "        -1.0466,  0.4744, -0.4180, -0.2603,  0.0328,  1.8920,  0.4395,  0.1396,\n",
      "         2.0044, -0.2098, -0.3497,  1.2236,  0.9438,  1.4236,  1.0467, -0.3646,\n",
      "         0.8104, -1.3170,  0.2625,  0.7512, -0.5917, -0.6232, -1.4947, -0.7965,\n",
      "        -0.0956,  0.2080,  0.4039,  0.0414, -0.4899, -0.8543,  0.4136,  0.6829,\n",
      "         0.3079,  0.0348,  0.6436, -0.2266, -0.2571, -1.0550, -2.7254,  1.8745,\n",
      "         0.1124, -0.2611,  0.4216, -1.3403,  0.4412, -0.5783,  0.0302,  1.1904,\n",
      "        -2.3998,  1.0777,  1.7465,  0.3461, -1.4319,  0.4730, -0.9022,  0.0432,\n",
      "         0.2136, -0.4850, -0.5014,  0.1306,  1.4221,  0.4800,  0.5223,  1.8127],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Word: 가격\n",
      "tensor([-0.8214,  0.3464, -2.2497, -0.4937, -0.3629, -1.9946,  0.7335,  1.0177,\n",
      "        -0.4437,  0.1554, -0.0957,  0.1707, -0.4398,  0.3971,  0.4582,  0.1587,\n",
      "        -0.2362, -1.9467,  1.4396, -0.9532,  0.7525,  0.4237,  0.7844,  0.9615,\n",
      "        -0.5161,  0.3744, -0.0405,  1.5255,  0.6467,  0.0050, -0.6056,  1.2890,\n",
      "        -0.3458,  0.5177,  3.0774, -0.1787, -2.4498, -1.6653,  0.7469, -0.9349,\n",
      "        -1.4945, -0.9201, -1.4807,  1.5218, -2.6101,  0.3609,  2.2206,  0.0721,\n",
      "        -0.3047, -1.6210,  0.0619,  1.8949,  0.3191, -1.0414, -0.1644,  0.0377,\n",
      "         0.2947, -1.2378,  1.5425,  0.2615,  0.8935,  1.0116,  1.0824,  1.0305,\n",
      "        -0.3441, -0.5019,  1.0345,  1.5144,  0.3517, -1.7487,  0.2133,  0.3799,\n",
      "         0.1888, -0.7739,  1.7471,  1.1555, -1.0576,  0.0435, -1.3942,  0.8971,\n",
      "        -0.8141, -1.0103, -1.8018, -0.3088, -2.0783,  0.2228,  1.1609,  0.8127,\n",
      "        -0.4722, -0.4629, -0.1537, -0.1788,  0.1012, -0.8603, -0.8288, -0.4559,\n",
      "         0.1527, -1.2472, -0.6242,  0.1075,  0.7665, -0.0604,  1.5783, -0.6912,\n",
      "        -0.5363,  0.1091, -1.1241,  1.4150, -0.5069, -0.8341,  0.1641,  0.6149,\n",
      "         1.0617, -1.7427,  0.0101, -0.6325,  1.8808,  1.3921, -1.4573,  0.9589,\n",
      "         0.0527,  0.1105,  0.3337,  2.6302,  0.8484, -0.5094, -1.6046,  0.6464,\n",
      "         0.6925, -0.8079, -0.2100, -0.0174, -0.4079, -0.8038, -0.8235,  1.5433,\n",
      "         0.5121, -0.2840, -1.1677, -0.3406,  0.0872, -2.2744, -1.7040, -1.6414,\n",
      "        -0.3377, -0.1632,  0.5808, -0.7413,  0.7511,  0.9841, -0.1432,  1.2129,\n",
      "         0.6332, -0.5375, -1.0694, -1.8005,  1.1735,  1.1100, -1.4891, -0.1166,\n",
      "         0.7442, -0.4707,  0.4737,  0.2725,  0.2787, -0.7929,  0.6065, -0.3936,\n",
      "        -1.2762, -0.3407, -2.1864, -0.3867,  0.6137,  1.0297, -0.7267,  0.2566,\n",
      "         0.4402, -1.2418,  0.2328, -1.0876,  1.3583, -0.2394,  0.2964, -0.0129,\n",
      "        -0.1398, -0.7274, -0.2794,  0.1395, -1.9114,  1.2468, -0.6060,  1.2007,\n",
      "        -1.4282, -1.1721,  0.3408,  0.3813, -1.6023,  1.1528, -0.1431,  0.3011,\n",
      "         0.6257, -0.1059,  0.7290,  0.7040, -0.3472,  0.4433,  1.0724,  0.4192,\n",
      "        -2.2503, -0.5432, -1.7113,  1.0139,  0.1217,  0.9354, -1.4698,  0.0588,\n",
      "        -0.1369,  1.1947, -0.2647, -1.7163, -1.3316, -0.6680, -0.4542,  1.1244,\n",
      "        -1.2153, -0.0605, -0.6578, -1.1239,  1.1358,  1.7846, -0.2952,  0.3494,\n",
      "        -0.4417,  1.4807, -0.7400, -0.2083, -1.3982,  0.1307,  0.1430,  0.1159,\n",
      "        -0.2131,  1.9756,  1.0949, -2.1401, -1.4400, -0.1033,  1.5440, -0.2374,\n",
      "        -1.3292, -0.1073, -0.4129,  0.3222, -0.9777, -0.5646, -0.4562,  0.4911],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
    "    emb = skipgram.embedding(input_id)\n",
    "    \n",
    "    print(f'Word: {word}')\n",
    "    print(emb.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-surrey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
