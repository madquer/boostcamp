{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "royal-crest",
   "metadata": {},
   "source": [
    "# LSTM, GRU, Language Modeling Task\n",
    "\n",
    "1. 기존 RNN 과 다른점\n",
    "2. 다양한 적용법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-deficit",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acquired-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-sarah",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "great-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "pad_id = 0\n",
    "\n",
    "data = [\n",
    "  [85,14,80,34,99,20,31,65,53,86,3,58,30,4,11,6,50,71,74,13],\n",
    "  [62,76,79,66,32],\n",
    "  [93,77,16,67,46,74,24,70],\n",
    "  [19,83,88,22,57,40,75,82,4,46],\n",
    "  [70,28,30,24,76,84,92,76,77,51,7,20,82,94,57],\n",
    "  [58,13,40,61,88,18,92,89,8,14,61,67,49,59,45,12,47,5],\n",
    "  [22,5,21,84,39,6,9,84,36,59,32,30,69,70,82,56,1],\n",
    "  [94,21,79,24,3,86],\n",
    "  [80,80,33,63,34,63],\n",
    "  [87,32,79,65,2,96,43,80,85,20,41,52,95,50,35,96,24,80]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "allied-tutorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 126334.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length : 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = len(max(data, key=len))\n",
    "print(f\"Maximum sequence length : {max_len}\")\n",
    "\n",
    "valid_lens = []\n",
    "for i, seq in enumerate(tqdm(data)):\n",
    "    valid_lens.append(len(seq))\n",
    "    if len(seq) < max_len:\n",
    "        data[i] = seq + [pad_id] * (max_len - len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continuing-meter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[85, 14, 80, 34, 99, 20, 31, 65, 53, 86,  3, 58, 30,  4, 11,  6, 50, 71,\n",
      "         74, 13],\n",
      "        [58, 13, 40, 61, 88, 18, 92, 89,  8, 14, 61, 67, 49, 59, 45, 12, 47,  5,\n",
      "          0,  0],\n",
      "        [87, 32, 79, 65,  2, 96, 43, 80, 85, 20, 41, 52, 95, 50, 35, 96, 24, 80,\n",
      "          0,  0],\n",
      "        [22,  5, 21, 84, 39,  6,  9, 84, 36, 59, 32, 30, 69, 70, 82, 56,  1,  0,\n",
      "          0,  0],\n",
      "        [70, 28, 30, 24, 76, 84, 92, 76, 77, 51,  7, 20, 82, 94, 57,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [19, 83, 88, 22, 57, 40, 75, 82,  4, 46,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [93, 77, 16, 67, 46, 74, 24, 70,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [94, 21, 79, 24,  3, 86,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [80, 80, 33, 63, 34, 63,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [62, 76, 79, 66, 32,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0]])\n",
      "tensor([20, 18, 18, 17, 15, 10,  8,  6,  6,  5])\n"
     ]
    }
   ],
   "source": [
    "# B : batch size, L : maximum sequence length\n",
    "\n",
    "batch = torch.LongTensor(data) # (B, L)\n",
    "batch_lens = torch.LongTensor(valid_lens) # (B)\n",
    "\n",
    "batch_lens, sorted_idx = batch_lens.sort(descending=True)\n",
    "batch = batch[sorted_idx]\n",
    "\n",
    "print(batch)\n",
    "print(batch_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-mining",
   "metadata": {},
   "source": [
    "## LSTM 사용\n",
    "\n",
    "LSTM 에선 cell state 가 추가됩니다.  \n",
    "Cell state 의 shape 는 hidden state 의 shape 와 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "increased-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "num_dirs = 1\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "lstm = nn.LSTM(\n",
    "    input_size = embedding_size,\n",
    "    hidden_size = hidden_size,\n",
    "    num_layers = num_layers,\n",
    "    bidirectional = True if num_dirs > 1 else False\n",
    ")\n",
    "\n",
    "# hidden vector\n",
    "h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size)) # (num_layers * num_dirs, B, d_h)\n",
    "\n",
    "# state vector\n",
    "c_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size)) # (num_layers * num_dirs, B, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "entertaining-amino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[-0.1417, -0.1051, -0.0269,  ..., -0.0809, -0.0321, -0.0338],\n",
      "        [ 0.0904, -0.1547, -0.0107,  ...,  0.0723,  0.0023,  0.1199],\n",
      "        [-0.1141, -0.0416, -0.0819,  ...,  0.0893,  0.0574, -0.0411],\n",
      "        ...,\n",
      "        [ 0.0612,  0.0333,  0.2206,  ...,  0.0219,  0.1055, -0.0301],\n",
      "        [-0.0178, -0.0548, -0.0342,  ...,  0.0118, -0.1112,  0.0629],\n",
      "        [-0.1250,  0.0660, -0.0872,  ...,  0.0182, -0.1434,  0.0069]],\n",
      "       grad_fn=<CatBackward>), batch_sizes=tensor([10, 10, 10, 10, 10,  9,  7,  7,  6,  6,  5,  5,  5,  5,  5,  4,  4,  3,\n",
      "         1,  1]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([123, 512])\n",
      "torch.Size([1, 10, 512])\n",
      "torch.Size([1, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# d_w : word embedding size\n",
    "batch_emb = embedding(batch) # (B, L, d_w) 임베딩\n",
    "\n",
    "packed_batch = pack_padded_sequence(batch_emb.transpose(0, 1), batch_lens) # pack sequence object\n",
    "\n",
    "packed_outputs, (h_n, c_n) = lstm(packed_batch, (h_0, c_0))\n",
    "print(packed_outputs)\n",
    "print(packed_outputs[0].shape)\n",
    "print(h_n.shape)\n",
    "print(c_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-copying",
   "metadata": {},
   "source": [
    "**torch.nn.utils.rnn.pack_padded_sequence**\n",
    "```py\n",
    "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)\n",
    "```\n",
    "\n",
    "`input (Tensor)` – padded batch of variable length sequences.\n",
    "\n",
    "`lengths (Tensor)` – list of sequences lengths of each batch element.\n",
    "\n",
    "`batch_first (bool, optional)` – if True, the input is expected in B x T x * format.\n",
    "\n",
    "`enforce_sorted (bool, optional)` – if True, the input is expected to contain sequences sorted by length in a decreasing order. If False, the input will get sorted unconditionally. Default: True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-ethiopia",
   "metadata": {},
   "source": [
    "**torch.nn.utils.rnn.pad_packed_sequence**\n",
    "\n",
    "```py\n",
    "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)\n",
    "```\n",
    "\n",
    "`sequence (PackedSequence)` – batch to pad\n",
    "\n",
    "`batch_first (bool, optional)` – if True, the output will be in B x T x * format.\n",
    "\n",
    "`padding_value (float, optional)` – values for padded elements.\n",
    "\n",
    "`total_length (int, optional)` – if not None, the output will be padded to have length total_length. This method will throw ValueError if total_length is less than the max sequence length in sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "closed-belfast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10, 512])\n",
      "tensor([20, 18, 18, 17, 15, 10,  8,  6,  6,  5])\n"
     ]
    }
   ],
   "source": [
    "outputs, output_lens = pad_packed_sequence(packed_outputs) # 다시 원래로 바꿔준다.\n",
    "print(outputs.shape)\n",
    "print(output_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-characteristic",
   "metadata": {},
   "source": [
    "## GRU 사용\n",
    "\n",
    "GRU 는 cell state 가 없어 RNN과 동일하게 사용 가능합니다.   \n",
    "GRU 를 이용하여 LM task를 수행해보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "finnish-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(\n",
    "    input_size = embedding_size,\n",
    "    hidden_size = hidden_size,\n",
    "    num_layers = num_layers,\n",
    "    bidirectional = True if num_dirs > 1 else False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "familiar-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = nn.Linear(hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mechanical-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id = batch.transpose(0,1)[0, :] # (B) # 첫 번째 단어만 가져온다.\n",
    "hidden = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size)) # (1, B, d_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-converter",
   "metadata": {},
   "source": [
    "**Teachear forcing**  \n",
    "\n",
    "초반에는 무조건 학습이 틀리기 때문에, 나온 결과가 다시 인풋으로 사용되는 것을 끊어버리고, 인위적으로 원래 데이터를 주입해주는 것.  \n",
    "\n",
    "Teacher forcing 없이 이전에 얻은 결과를 다음 input 으로 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sunset-david",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Time step: 0\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 1\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 2\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 3\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 4\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 5\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 6\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 7\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 8\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 9\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 10\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 11\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 12\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 13\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 14\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 15\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 16\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 17\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 18\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "**************************************************\n",
      "Time step: 19\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# language modeling 할 때는 직접 for roop 를 통해서 넣어준다.\n",
    "\n",
    "for t in range(max_len):\n",
    "    input_emb = embedding(input_id).unsqueeze(0) # (1, B, d_w)\n",
    "    output, hidden = gru(input_emb, hidden) # output : (1, B, d_h), hidden : (1, B, d_h)\n",
    "    \n",
    "    # V : vocab size\n",
    "    output = output_layer(output) # (1, B, V)\n",
    "    probs, top_id = torch.max(output, dim=-1) # probs : (1, B), top_id : (1, B)\n",
    "    \n",
    "    print('*' * 50)\n",
    "    print(f\"Time step: {t}\")\n",
    "    print(output.shape)\n",
    "    print(probs.shape)\n",
    "    print(top_id.shape)\n",
    "    \n",
    "    input_id = top_id.squeeze(0) # (B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-conviction",
   "metadata": {},
   "source": [
    "`max_len` 만큼의 for 문을 돌면서 모든 결과물의 모양을 확인했지만 만약 종료 조건(예를 들어 문장의 끝을 나타내는 end token 등) 이 되면  \n",
    "중간에 생성을 그만둘 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-durham",
   "metadata": {},
   "source": [
    "## 양방향 및 여러 layer 사용\n",
    "\n",
    "이번엔 양방향 + 2개 이상의 layer 를 쓸 때 얻을 수 있는 결과에 대해 알아봅니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "later-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "num_dirs = 2\n",
    "dropout = 0.1\n",
    "\n",
    "gru = nn.GRU(\n",
    "    input_size = embedding_size,\n",
    "    hidden_size = hidden_size,\n",
    "    num_layers = num_layers,\n",
    "    dropout = dropout,\n",
    "    bidirectional = True if num_dirs > 1 else False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-staff",
   "metadata": {},
   "source": [
    "Bidirectional 이 되었고 layer 의 개수가 2로 늘었기 때문에 hidden state 의 shape 도 `(4, B, d_h)` 가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "psychological-beijing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[-0.0268, -0.0156, -0.0764,  ..., -0.0013, -0.0648,  0.0530],\n",
      "        [-0.0207,  0.0174, -0.0403,  ...,  0.2451, -0.0301,  0.2327],\n",
      "        [-0.0364, -0.0895, -0.0029,  ...,  0.2567, -0.0595,  0.0529],\n",
      "        ...,\n",
      "        [ 0.1644, -0.0686,  0.0315,  ..., -0.0315,  0.0845,  0.0524],\n",
      "        [ 0.0934,  0.0764,  0.1168,  ..., -0.1100, -0.1745, -0.0697],\n",
      "        [-0.0163,  0.1236,  0.1755,  ...,  0.0770,  0.0490,  0.0528]],\n",
      "       grad_fn=<CatBackward>), batch_sizes=tensor([10, 10, 10, 10, 10,  9,  7,  7,  6,  6,  5,  5,  5,  5,  5,  4,  4,  3,\n",
      "         1,  1]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([123, 1024])\n",
      "torch.Size([4, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# d_w : word embedding size, num_layers : layer 의 개수, num_dirs : 방향의 개수\n",
    "batch_emb = embedding(batch) # (B, L, d_w)\n",
    "h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size)) # (num_layers * num_dirs, B, d_h) = (4, B, d_h)\n",
    "\n",
    "packed_batch = pack_padded_sequence(batch_emb.transpose(0,1), batch_lens)\n",
    "\n",
    "packed_outputs, h_n = gru(packed_batch, h_0)\n",
    "print(packed_outputs)\n",
    "print(packed_outputs[0].shape)\n",
    "print(h_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "reverse-revolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10, 1024])\n",
      "tensor([20, 18, 18, 17, 15, 10,  8,  6,  6,  5])\n"
     ]
    }
   ],
   "source": [
    "outputs, output_lens = pad_packed_sequence(packed_outputs)\n",
    "\n",
    "print(outputs.shape) # (L, B, num_dirs * d_h)\n",
    "print(output_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-influence",
   "metadata": {},
   "source": [
    "각각의 결과물의 shape 는 다음과 같습니다.\n",
    "\n",
    "`outputs : (max_len, batch_size, num_dir * hidden_size)`  \n",
    "`h_n : (num_layer * num_dirs, batch_size, hidden_size)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "incomplete-spice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2623, -0.1769, -0.1118,  ..., -0.0034, -0.1100, -0.0783],\n",
      "          [ 0.0996,  0.2707, -0.1935,  ...,  0.2685,  0.1765, -0.0115],\n",
      "          [ 0.3811, -0.2315,  0.1433,  ..., -0.0525,  0.0680, -0.2746],\n",
      "          ...,\n",
      "          [-0.0486,  0.0248,  0.0622,  ..., -0.0385,  0.3415, -0.3320],\n",
      "          [-0.2905,  0.0195, -0.0787,  ..., -0.0494, -0.2308,  0.2295],\n",
      "          [ 0.0376,  0.1778, -0.1356,  ...,  0.2565, -0.0275, -0.3093]],\n",
      "\n",
      "         [[-0.3307,  0.1377, -0.1100,  ..., -0.1522, -0.1077,  0.0790],\n",
      "          [-0.1431, -0.2494,  0.2846,  ...,  0.2209,  0.2795, -0.0268],\n",
      "          [-0.4334, -0.4231, -0.0604,  ..., -0.2446,  0.2793,  0.0018],\n",
      "          ...,\n",
      "          [-0.0895,  0.2337, -0.0922,  ...,  0.1417, -0.1363, -0.0085],\n",
      "          [-0.1658,  0.1323, -0.4728,  ..., -0.2972,  0.0161,  0.1540],\n",
      "          [ 0.1088, -0.1732,  0.0303,  ...,  0.0151, -0.3359,  0.1943]]],\n",
      "\n",
      "\n",
      "        [[[-0.0163,  0.1236,  0.1755,  ..., -0.1768,  0.0082,  0.0333],\n",
      "          [-0.2539, -0.0231,  0.0095,  ...,  0.0358,  0.1094,  0.3013],\n",
      "          [ 0.1644, -0.0686,  0.0315,  ..., -0.0062, -0.1626, -0.0471],\n",
      "          ...,\n",
      "          [ 0.0214,  0.0150, -0.1115,  ..., -0.1056,  0.0037, -0.2090],\n",
      "          [ 0.1603, -0.0894,  0.0132,  ..., -0.1617, -0.0184,  0.0401],\n",
      "          [ 0.0176, -0.4170,  0.1063,  ...,  0.0432,  0.0995, -0.0124]],\n",
      "\n",
      "         [[ 0.1305, -0.1551, -0.0528,  ..., -0.0013, -0.0648,  0.0530],\n",
      "          [ 0.0015, -0.0744, -0.1537,  ...,  0.2451, -0.0301,  0.2327],\n",
      "          [ 0.1835, -0.1348,  0.0058,  ...,  0.2567, -0.0595,  0.0529],\n",
      "          ...,\n",
      "          [ 0.0054, -0.1749, -0.0820,  ...,  0.1789, -0.0840,  0.1532],\n",
      "          [ 0.1577,  0.2086,  0.3750,  ..., -0.0953,  0.0905,  0.1157],\n",
      "          [-0.0015, -0.2410, -0.0618,  ...,  0.1482,  0.1170,  0.2527]]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "torch.Size([2, 2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_size = h_n.shape[1]\n",
    "print(h_n.view(num_layers, num_dirs, batch_size, hidden_size))\n",
    "print(h_n.view(num_layers, num_dirs, batch_size, hidden_size).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-winning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
