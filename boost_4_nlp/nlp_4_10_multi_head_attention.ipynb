{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efficient-dominant",
   "metadata": {},
   "source": [
    "# Multi-head Attention\n",
    "\n",
    "1. Multi-head attention 및 self-attention 구현\n",
    "2. 각 과정에서 일어나는 연산과 input/output 형태 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-passage",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "turkish-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-simpson",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finite-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = 0\n",
    "vocab_size = 100\n",
    "\n",
    "data = [\n",
    "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
    "  [60, 96, 51, 32, 90],\n",
    "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
    "  [75, 51],\n",
    "  [66, 88, 98, 47],\n",
    "  [21, 39, 10, 64, 21],\n",
    "  [98],\n",
    "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
    "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
    "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unauthorized-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(data):\n",
    "    max_len = len(max(data, key=len))\n",
    "    print(f\"Maximum sequence length : {max_len}\")\n",
    "    \n",
    "    for i, seq in enumerate(tqdm(data)):\n",
    "        if len(seq) < max_len:\n",
    "            data[i] = seq + [pad_id] * (max_len - len(seq))\n",
    "            \n",
    "    return data, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prerequisite-testing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 137068.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length : 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data, max_len = padding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "underlying-original",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
       " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [77,\n",
       "  65,\n",
       "  51,\n",
       "  77,\n",
       "  19,\n",
       "  15,\n",
       "  35,\n",
       "  19,\n",
       "  23,\n",
       "  97,\n",
       "  50,\n",
       "  46,\n",
       "  53,\n",
       "  42,\n",
       "  45,\n",
       "  91,\n",
       "  66,\n",
       "  3,\n",
       "  43,\n",
       "  10],\n",
       " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
       " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-disability",
   "metadata": {},
   "source": [
    "## Hyperparameter 세팅 및 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accomplished-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512 # model 의 hidden size\n",
    "num_heads = 8 # head 의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "residential-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "# B : batch size, L : maximum sequnce length\n",
    "batch = torch.LongTensor(data) # (B, L)\n",
    "batch_emb = embedding(batch) # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stainless-disabled",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75,  0,  0,\n",
      "          0,  0],\n",
      "        [60, 96, 51, 32, 90,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [35, 45, 48, 65, 91, 99, 92, 10,  3, 21, 54,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [75, 51,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [66, 88, 98, 47,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [21, 39, 10, 64, 21,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [98,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66,  3,\n",
      "         43, 10],\n",
      "        [70, 64, 98, 25, 99, 53,  4, 13, 69, 62, 66, 76, 15, 75, 45, 34,  0,  0,\n",
      "          0,  0],\n",
      "        [20, 64, 81, 35, 76, 85,  1, 62,  8, 45, 99, 77, 19, 43,  0,  0,  0,  0,\n",
      "          0,  0]])\n",
      "tensor([[[ 0.9822,  0.6481,  0.2682,  ...,  0.3026, -0.2204,  0.4892],\n",
      "         [ 0.4924, -2.0706,  0.2313,  ..., -0.1163, -1.4590, -1.6560],\n",
      "         [-1.3121, -0.6675,  0.3196,  ...,  0.0933,  1.1532, -0.3545],\n",
      "         ...,\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763],\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763],\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763]],\n",
      "\n",
      "        [[ 0.0135, -0.9849,  0.2759,  ...,  0.2031, -0.1964, -1.5173],\n",
      "         [-0.2258,  1.7220, -0.1173,  ...,  0.8117,  1.0380, -0.7936],\n",
      "         [ 1.5124, -0.4312, -0.2650,  ...,  0.1364, -0.6757,  0.4320],\n",
      "         ...,\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763],\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763],\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763]],\n",
      "\n",
      "        [[ 1.8926, -0.7030, -1.2076,  ..., -0.3776, -1.4236, -1.4515],\n",
      "         [ 1.4786, -2.1206, -0.2860,  ...,  0.2321,  0.2871,  0.5343],\n",
      "         [-0.5286,  1.5276,  0.6576,  ..., -0.2746,  0.8964,  0.1043],\n",
      "         ...,\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763],\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763],\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3478, -1.0748, -0.7556,  ...,  0.8257, -1.9068, -0.7514],\n",
      "         [ 1.2981,  0.5001,  0.9682,  ...,  1.0231, -0.9415,  0.3624],\n",
      "         [ 1.5124, -0.4312, -0.2650,  ...,  0.1364, -0.6757,  0.4320],\n",
      "         ...,\n",
      "         [-0.9581, -0.8125, -0.5911,  ..., -1.8028, -0.8366,  2.2510],\n",
      "         [ 1.4544, -0.7108, -0.8583,  ..., -1.0819, -2.1414, -1.2294],\n",
      "         [-1.5361,  1.4179, -0.1261,  ...,  2.0851, -0.1878, -0.1250]],\n",
      "\n",
      "        [[ 0.0862,  1.4613,  0.3598,  ..., -0.8742,  0.5671, -0.6053],\n",
      "         [ 0.1775, -0.5114, -1.0628,  ...,  3.3295,  1.7286,  0.8216],\n",
      "         [-0.1183, -0.1321, -1.3847,  ...,  0.0153, -0.2169,  0.6715],\n",
      "         ...,\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763],\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763],\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763]],\n",
      "\n",
      "        [[ 0.6053,  0.6791,  0.5168,  ..., -0.4577,  0.7563, -0.5158],\n",
      "         [ 0.1775, -0.5114, -1.0628,  ...,  3.3295,  1.7286,  0.8216],\n",
      "         [-0.6175,  0.5096, -0.1510,  ..., -0.4982,  1.2178,  0.1412],\n",
      "         ...,\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763],\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763],\n",
      "         [-0.9554, -1.5686, -1.2698,  ..., -0.0548,  0.2963,  0.8763]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(batch)\n",
    "print(batch_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-finnish",
   "metadata": {},
   "source": [
    "## Linear transformation & 여러 head 로 나누기\n",
    "\n",
    "Multi-head attention 내에서 쓰이는 linear transformation matrix 들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "electrical-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_q = nn.Linear(d_model, d_model)\n",
    "w_k = nn.Linear(d_model, d_model)\n",
    "w_v = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "infinite-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "overall-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "judicial-aluminum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "q = w_q(batch_emb) # (B, L, d_model)\n",
    "k = w_k(batch_emb) # (B, L, d_model)\n",
    "v = w_v(batch_emb) # (B, L, d_model)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-karen",
   "metadata": {},
   "source": [
    "Q, K, V 를 `num_head` 개의 차원 분할된 여러 vector 로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "advance-assist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = q.shape[0]\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "complete-bradford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 20, 64])\n",
      "torch.Size([10, 8, 20, 64])\n",
      "torch.Size([10, 8, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-syndicate",
   "metadata": {},
   "source": [
    "## Scaled dot-product self-attention 구현\n",
    "\n",
    "각 head에서 실행되는 self-attention 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "weird-stephen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0699, 0.0504, 0.0650,  ..., 0.0240, 0.0240, 0.0240],\n",
      "          [0.0390, 0.0360, 0.0306,  ..., 0.0468, 0.0468, 0.0468],\n",
      "          [0.0395, 0.0448, 0.0584,  ..., 0.0456, 0.0456, 0.0456],\n",
      "          ...,\n",
      "          [0.0339, 0.0286, 0.0389,  ..., 0.0845, 0.0845, 0.0845],\n",
      "          [0.0339, 0.0286, 0.0389,  ..., 0.0845, 0.0845, 0.0845],\n",
      "          [0.0339, 0.0286, 0.0389,  ..., 0.0845, 0.0845, 0.0845]],\n",
      "\n",
      "         [[0.1091, 0.0719, 0.0330,  ..., 0.0496, 0.0496, 0.0496],\n",
      "          [0.0264, 0.0488, 0.0625,  ..., 0.0522, 0.0522, 0.0522],\n",
      "          [0.0280, 0.0555, 0.0380,  ..., 0.0912, 0.0912, 0.0912],\n",
      "          ...,\n",
      "          [0.0367, 0.0368, 0.0342,  ..., 0.0537, 0.0537, 0.0537],\n",
      "          [0.0367, 0.0368, 0.0342,  ..., 0.0537, 0.0537, 0.0537],\n",
      "          [0.0367, 0.0368, 0.0342,  ..., 0.0537, 0.0537, 0.0537]],\n",
      "\n",
      "         [[0.0822, 0.0392, 0.0309,  ..., 0.0501, 0.0501, 0.0501],\n",
      "          [0.0572, 0.0349, 0.0709,  ..., 0.0453, 0.0453, 0.0453],\n",
      "          [0.0317, 0.0567, 0.0387,  ..., 0.0526, 0.0526, 0.0526],\n",
      "          ...,\n",
      "          [0.0367, 0.0457, 0.0400,  ..., 0.0474, 0.0474, 0.0474],\n",
      "          [0.0367, 0.0457, 0.0400,  ..., 0.0474, 0.0474, 0.0474],\n",
      "          [0.0367, 0.0457, 0.0400,  ..., 0.0474, 0.0474, 0.0474]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0401, 0.0539, 0.0441,  ..., 0.0237, 0.0237, 0.0237],\n",
      "          [0.0548, 0.0466, 0.0868,  ..., 0.0358, 0.0358, 0.0358],\n",
      "          [0.0320, 0.0580, 0.0508,  ..., 0.0639, 0.0639, 0.0639],\n",
      "          ...,\n",
      "          [0.0438, 0.0403, 0.0817,  ..., 0.0583, 0.0583, 0.0583],\n",
      "          [0.0438, 0.0403, 0.0817,  ..., 0.0583, 0.0583, 0.0583],\n",
      "          [0.0438, 0.0403, 0.0817,  ..., 0.0583, 0.0583, 0.0583]],\n",
      "\n",
      "         [[0.0436, 0.0762, 0.0362,  ..., 0.0244, 0.0244, 0.0244],\n",
      "          [0.0283, 0.0483, 0.0737,  ..., 0.0452, 0.0452, 0.0452],\n",
      "          [0.0344, 0.0408, 0.0730,  ..., 0.0702, 0.0702, 0.0702],\n",
      "          ...,\n",
      "          [0.0594, 0.0566, 0.0442,  ..., 0.0415, 0.0415, 0.0415],\n",
      "          [0.0594, 0.0566, 0.0442,  ..., 0.0415, 0.0415, 0.0415],\n",
      "          [0.0594, 0.0566, 0.0442,  ..., 0.0415, 0.0415, 0.0415]],\n",
      "\n",
      "         [[0.0326, 0.0572, 0.0401,  ..., 0.0376, 0.0376, 0.0376],\n",
      "          [0.0475, 0.0512, 0.0437,  ..., 0.0589, 0.0589, 0.0589],\n",
      "          [0.0338, 0.0518, 0.0299,  ..., 0.0588, 0.0588, 0.0588],\n",
      "          ...,\n",
      "          [0.0448, 0.0540, 0.0583,  ..., 0.0510, 0.0510, 0.0510],\n",
      "          [0.0448, 0.0540, 0.0583,  ..., 0.0510, 0.0510, 0.0510],\n",
      "          [0.0448, 0.0540, 0.0583,  ..., 0.0510, 0.0510, 0.0510]]],\n",
      "\n",
      "\n",
      "        [[[0.0278, 0.0476, 0.0285,  ..., 0.0549, 0.0549, 0.0549],\n",
      "          [0.0916, 0.0646, 0.0818,  ..., 0.0404, 0.0404, 0.0404],\n",
      "          [0.0527, 0.0714, 0.0699,  ..., 0.0463, 0.0463, 0.0463],\n",
      "          ...,\n",
      "          [0.0361, 0.0230, 0.0276,  ..., 0.0567, 0.0567, 0.0567],\n",
      "          [0.0361, 0.0230, 0.0276,  ..., 0.0567, 0.0567, 0.0567],\n",
      "          [0.0361, 0.0230, 0.0276,  ..., 0.0567, 0.0567, 0.0567]],\n",
      "\n",
      "         [[0.0448, 0.0566, 0.0785,  ..., 0.0477, 0.0477, 0.0477],\n",
      "          [0.0281, 0.0478, 0.0515,  ..., 0.0474, 0.0474, 0.0474],\n",
      "          [0.0510, 0.0564, 0.0704,  ..., 0.0477, 0.0477, 0.0477],\n",
      "          ...,\n",
      "          [0.0326, 0.0384, 0.0547,  ..., 0.0519, 0.0519, 0.0519],\n",
      "          [0.0326, 0.0384, 0.0547,  ..., 0.0519, 0.0519, 0.0519],\n",
      "          [0.0326, 0.0384, 0.0547,  ..., 0.0519, 0.0519, 0.0519]],\n",
      "\n",
      "         [[0.0385, 0.0755, 0.0462,  ..., 0.0487, 0.0487, 0.0487],\n",
      "          [0.0469, 0.0537, 0.0604,  ..., 0.0512, 0.0512, 0.0512],\n",
      "          [0.0509, 0.0401, 0.0486,  ..., 0.0513, 0.0513, 0.0513],\n",
      "          ...,\n",
      "          [0.0715, 0.0304, 0.0371,  ..., 0.0504, 0.0504, 0.0504],\n",
      "          [0.0715, 0.0304, 0.0371,  ..., 0.0504, 0.0504, 0.0504],\n",
      "          [0.0715, 0.0304, 0.0371,  ..., 0.0504, 0.0504, 0.0504]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0613, 0.0627, 0.0802,  ..., 0.0416, 0.0416, 0.0416],\n",
      "          [0.0377, 0.0513, 0.0597,  ..., 0.0526, 0.0526, 0.0526],\n",
      "          [0.0458, 0.0530, 0.0316,  ..., 0.0480, 0.0480, 0.0480],\n",
      "          ...,\n",
      "          [0.0605, 0.0517, 0.0633,  ..., 0.0513, 0.0513, 0.0513],\n",
      "          [0.0605, 0.0517, 0.0633,  ..., 0.0513, 0.0513, 0.0513],\n",
      "          [0.0605, 0.0517, 0.0633,  ..., 0.0513, 0.0513, 0.0513]],\n",
      "\n",
      "         [[0.0537, 0.0780, 0.0665,  ..., 0.0478, 0.0478, 0.0478],\n",
      "          [0.0431, 0.0322, 0.0498,  ..., 0.0530, 0.0530, 0.0530],\n",
      "          [0.0866, 0.0671, 0.0494,  ..., 0.0458, 0.0458, 0.0458],\n",
      "          ...,\n",
      "          [0.0685, 0.0658, 0.0623,  ..., 0.0459, 0.0459, 0.0459],\n",
      "          [0.0685, 0.0658, 0.0623,  ..., 0.0459, 0.0459, 0.0459],\n",
      "          [0.0685, 0.0658, 0.0623,  ..., 0.0459, 0.0459, 0.0459]],\n",
      "\n",
      "         [[0.0550, 0.0449, 0.0633,  ..., 0.0491, 0.0491, 0.0491],\n",
      "          [0.0437, 0.0651, 0.0839,  ..., 0.0489, 0.0489, 0.0489],\n",
      "          [0.0716, 0.0537, 0.0553,  ..., 0.0439, 0.0439, 0.0439],\n",
      "          ...,\n",
      "          [0.0544, 0.0245, 0.0708,  ..., 0.0495, 0.0495, 0.0495],\n",
      "          [0.0544, 0.0245, 0.0708,  ..., 0.0495, 0.0495, 0.0495],\n",
      "          [0.0544, 0.0245, 0.0708,  ..., 0.0495, 0.0495, 0.0495]]],\n",
      "\n",
      "\n",
      "        [[[0.0770, 0.0565, 0.0327,  ..., 0.0473, 0.0473, 0.0473],\n",
      "          [0.0642, 0.0367, 0.0471,  ..., 0.0510, 0.0510, 0.0510],\n",
      "          [0.0368, 0.0361, 0.0569,  ..., 0.0573, 0.0573, 0.0573],\n",
      "          ...,\n",
      "          [0.0436, 0.0357, 0.0680,  ..., 0.0616, 0.0616, 0.0616],\n",
      "          [0.0436, 0.0357, 0.0680,  ..., 0.0616, 0.0616, 0.0616],\n",
      "          [0.0436, 0.0357, 0.0680,  ..., 0.0616, 0.0616, 0.0616]],\n",
      "\n",
      "         [[0.0413, 0.0297, 0.0603,  ..., 0.0614, 0.0614, 0.0614],\n",
      "          [0.0649, 0.0655, 0.0587,  ..., 0.0438, 0.0438, 0.0438],\n",
      "          [0.0424, 0.0331, 0.1055,  ..., 0.0404, 0.0404, 0.0404],\n",
      "          ...,\n",
      "          [0.0437, 0.0409, 0.0423,  ..., 0.0500, 0.0500, 0.0500],\n",
      "          [0.0437, 0.0409, 0.0423,  ..., 0.0500, 0.0500, 0.0500],\n",
      "          [0.0437, 0.0409, 0.0423,  ..., 0.0500, 0.0500, 0.0500]],\n",
      "\n",
      "         [[0.0539, 0.0373, 0.0523,  ..., 0.0525, 0.0525, 0.0525],\n",
      "          [0.0677, 0.0468, 0.0474,  ..., 0.0469, 0.0469, 0.0469],\n",
      "          [0.0503, 0.0639, 0.0528,  ..., 0.0371, 0.0371, 0.0371],\n",
      "          ...,\n",
      "          [0.0595, 0.0276, 0.0636,  ..., 0.0483, 0.0483, 0.0483],\n",
      "          [0.0595, 0.0276, 0.0636,  ..., 0.0483, 0.0483, 0.0483],\n",
      "          [0.0595, 0.0276, 0.0636,  ..., 0.0483, 0.0483, 0.0483]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0280, 0.0390, 0.0346,  ..., 0.0665, 0.0665, 0.0665],\n",
      "          [0.0552, 0.0287, 0.0524,  ..., 0.0521, 0.0521, 0.0521],\n",
      "          [0.0484, 0.0378, 0.0743,  ..., 0.0514, 0.0514, 0.0514],\n",
      "          ...,\n",
      "          [0.0367, 0.0343, 0.0345,  ..., 0.0516, 0.0516, 0.0516],\n",
      "          [0.0367, 0.0343, 0.0345,  ..., 0.0516, 0.0516, 0.0516],\n",
      "          [0.0367, 0.0343, 0.0345,  ..., 0.0516, 0.0516, 0.0516]],\n",
      "\n",
      "         [[0.0394, 0.0811, 0.0632,  ..., 0.0337, 0.0337, 0.0337],\n",
      "          [0.0580, 0.0704, 0.0385,  ..., 0.0419, 0.0419, 0.0419],\n",
      "          [0.0581, 0.0501, 0.0902,  ..., 0.0387, 0.0387, 0.0387],\n",
      "          ...,\n",
      "          [0.0938, 0.0390, 0.0663,  ..., 0.0432, 0.0432, 0.0432],\n",
      "          [0.0938, 0.0390, 0.0663,  ..., 0.0432, 0.0432, 0.0432],\n",
      "          [0.0938, 0.0390, 0.0663,  ..., 0.0432, 0.0432, 0.0432]],\n",
      "\n",
      "         [[0.0750, 0.0224, 0.0737,  ..., 0.0390, 0.0390, 0.0390],\n",
      "          [0.0344, 0.0506, 0.0619,  ..., 0.0456, 0.0456, 0.0456],\n",
      "          [0.0341, 0.0259, 0.0322,  ..., 0.0639, 0.0639, 0.0639],\n",
      "          ...,\n",
      "          [0.0534, 0.0496, 0.0622,  ..., 0.0553, 0.0553, 0.0553],\n",
      "          [0.0534, 0.0496, 0.0622,  ..., 0.0553, 0.0553, 0.0553],\n",
      "          [0.0534, 0.0496, 0.0622,  ..., 0.0553, 0.0553, 0.0553]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0548, 0.0873, 0.0509,  ..., 0.0464, 0.0447, 0.0681],\n",
      "          [0.0457, 0.0415, 0.0429,  ..., 0.0755, 0.0386, 0.0476],\n",
      "          [0.0460, 0.0340, 0.0538,  ..., 0.0569, 0.0424, 0.0381],\n",
      "          ...,\n",
      "          [0.0722, 0.0437, 0.0363,  ..., 0.0419, 0.0311, 0.0490],\n",
      "          [0.0675, 0.0449, 0.0540,  ..., 0.0361, 0.0335, 0.0290],\n",
      "          [0.0831, 0.0593, 0.0393,  ..., 0.0309, 0.0203, 0.0630]],\n",
      "\n",
      "         [[0.0571, 0.0780, 0.0228,  ..., 0.0449, 0.0670, 0.1005],\n",
      "          [0.0939, 0.0347, 0.0525,  ..., 0.0504, 0.0322, 0.0422],\n",
      "          [0.0300, 0.0488, 0.0635,  ..., 0.0330, 0.0591, 0.0392],\n",
      "          ...,\n",
      "          [0.0196, 0.0466, 0.0860,  ..., 0.0549, 0.0622, 0.0770],\n",
      "          [0.0357, 0.0434, 0.0802,  ..., 0.0345, 0.0288, 0.0662],\n",
      "          [0.0816, 0.0515, 0.0323,  ..., 0.0257, 0.0610, 0.0707]],\n",
      "\n",
      "         [[0.0404, 0.0825, 0.0417,  ..., 0.0254, 0.0686, 0.0698],\n",
      "          [0.0656, 0.0328, 0.0584,  ..., 0.0510, 0.0454, 0.0246],\n",
      "          [0.0479, 0.0405, 0.0455,  ..., 0.0428, 0.0661, 0.0656],\n",
      "          ...,\n",
      "          [0.0642, 0.0842, 0.0443,  ..., 0.0271, 0.0464, 0.0487],\n",
      "          [0.0541, 0.0516, 0.0354,  ..., 0.0719, 0.0210, 0.0459],\n",
      "          [0.0399, 0.0484, 0.0480,  ..., 0.0317, 0.0521, 0.0612]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0879, 0.0402, 0.0538,  ..., 0.0757, 0.0217, 0.0424],\n",
      "          [0.0551, 0.0307, 0.0419,  ..., 0.0628, 0.0495, 0.0692],\n",
      "          [0.0627, 0.0212, 0.0232,  ..., 0.0514, 0.0375, 0.0645],\n",
      "          ...,\n",
      "          [0.0553, 0.0507, 0.0827,  ..., 0.0369, 0.0686, 0.0460],\n",
      "          [0.0570, 0.0539, 0.0668,  ..., 0.0401, 0.0357, 0.0988],\n",
      "          [0.0675, 0.0503, 0.0699,  ..., 0.0446, 0.0474, 0.0383]],\n",
      "\n",
      "         [[0.0287, 0.0495, 0.0403,  ..., 0.0723, 0.0341, 0.0674],\n",
      "          [0.0488, 0.0404, 0.0383,  ..., 0.0325, 0.0579, 0.0516],\n",
      "          [0.0656, 0.0410, 0.0294,  ..., 0.0290, 0.0644, 0.0584],\n",
      "          ...,\n",
      "          [0.0646, 0.0604, 0.0370,  ..., 0.0332, 0.0737, 0.0390],\n",
      "          [0.0497, 0.0697, 0.0524,  ..., 0.0622, 0.0393, 0.0656],\n",
      "          [0.0702, 0.0482, 0.0309,  ..., 0.0237, 0.0387, 0.0364]],\n",
      "\n",
      "         [[0.0539, 0.0326, 0.0839,  ..., 0.0384, 0.0816, 0.0816],\n",
      "          [0.0630, 0.0408, 0.0495,  ..., 0.0327, 0.0337, 0.0454],\n",
      "          [0.0506, 0.0662, 0.0319,  ..., 0.0454, 0.0522, 0.0681],\n",
      "          ...,\n",
      "          [0.0709, 0.0444, 0.0340,  ..., 0.0777, 0.0395, 0.0457],\n",
      "          [0.0477, 0.0701, 0.0366,  ..., 0.0307, 0.0287, 0.0303],\n",
      "          [0.0736, 0.0638, 0.0641,  ..., 0.0594, 0.0395, 0.0810]]],\n",
      "\n",
      "\n",
      "        [[[0.0446, 0.0475, 0.0620,  ..., 0.0357, 0.0357, 0.0357],\n",
      "          [0.0223, 0.0355, 0.0676,  ..., 0.0434, 0.0434, 0.0434],\n",
      "          [0.0813, 0.0379, 0.0452,  ..., 0.0719, 0.0719, 0.0719],\n",
      "          ...,\n",
      "          [0.0508, 0.0537, 0.0342,  ..., 0.0782, 0.0782, 0.0782],\n",
      "          [0.0508, 0.0537, 0.0342,  ..., 0.0782, 0.0782, 0.0782],\n",
      "          [0.0508, 0.0537, 0.0342,  ..., 0.0782, 0.0782, 0.0782]],\n",
      "\n",
      "         [[0.0329, 0.0451, 0.0517,  ..., 0.0518, 0.0518, 0.0518],\n",
      "          [0.0447, 0.0414, 0.1266,  ..., 0.0417, 0.0417, 0.0417],\n",
      "          [0.0445, 0.0550, 0.0363,  ..., 0.0667, 0.0667, 0.0667],\n",
      "          ...,\n",
      "          [0.0532, 0.0407, 0.0327,  ..., 0.0498, 0.0498, 0.0498],\n",
      "          [0.0532, 0.0407, 0.0327,  ..., 0.0498, 0.0498, 0.0498],\n",
      "          [0.0532, 0.0407, 0.0327,  ..., 0.0498, 0.0498, 0.0498]],\n",
      "\n",
      "         [[0.0610, 0.0681, 0.0513,  ..., 0.0289, 0.0289, 0.0289],\n",
      "          [0.0442, 0.0592, 0.0755,  ..., 0.0424, 0.0424, 0.0424],\n",
      "          [0.0964, 0.0895, 0.0325,  ..., 0.0527, 0.0527, 0.0527],\n",
      "          ...,\n",
      "          [0.0431, 0.0292, 0.0575,  ..., 0.0551, 0.0551, 0.0551],\n",
      "          [0.0431, 0.0292, 0.0575,  ..., 0.0551, 0.0551, 0.0551],\n",
      "          [0.0431, 0.0292, 0.0575,  ..., 0.0551, 0.0551, 0.0551]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0239, 0.0769, 0.0707,  ..., 0.0304, 0.0304, 0.0304],\n",
      "          [0.0463, 0.0657, 0.0909,  ..., 0.0457, 0.0457, 0.0457],\n",
      "          [0.0539, 0.0379, 0.0743,  ..., 0.0342, 0.0342, 0.0342],\n",
      "          ...,\n",
      "          [0.0493, 0.0583, 0.0361,  ..., 0.0552, 0.0552, 0.0552],\n",
      "          [0.0493, 0.0583, 0.0361,  ..., 0.0552, 0.0552, 0.0552],\n",
      "          [0.0493, 0.0583, 0.0361,  ..., 0.0552, 0.0552, 0.0552]],\n",
      "\n",
      "         [[0.0797, 0.0423, 0.0588,  ..., 0.0306, 0.0306, 0.0306],\n",
      "          [0.0562, 0.0366, 0.0414,  ..., 0.0333, 0.0333, 0.0333],\n",
      "          [0.0315, 0.0496, 0.0242,  ..., 0.0569, 0.0569, 0.0569],\n",
      "          ...,\n",
      "          [0.0607, 0.0702, 0.0500,  ..., 0.0393, 0.0393, 0.0393],\n",
      "          [0.0607, 0.0702, 0.0500,  ..., 0.0393, 0.0393, 0.0393],\n",
      "          [0.0607, 0.0702, 0.0500,  ..., 0.0393, 0.0393, 0.0393]],\n",
      "\n",
      "         [[0.0437, 0.0471, 0.0370,  ..., 0.0389, 0.0389, 0.0389],\n",
      "          [0.0351, 0.0571, 0.0739,  ..., 0.0368, 0.0368, 0.0368],\n",
      "          [0.0486, 0.0787, 0.0321,  ..., 0.0475, 0.0475, 0.0475],\n",
      "          ...,\n",
      "          [0.0262, 0.0374, 0.0423,  ..., 0.0592, 0.0592, 0.0592],\n",
      "          [0.0262, 0.0374, 0.0423,  ..., 0.0592, 0.0592, 0.0592],\n",
      "          [0.0262, 0.0374, 0.0423,  ..., 0.0592, 0.0592, 0.0592]]],\n",
      "\n",
      "\n",
      "        [[[0.0486, 0.0509, 0.0425,  ..., 0.0458, 0.0458, 0.0458],\n",
      "          [0.0486, 0.0352, 0.0351,  ..., 0.0430, 0.0430, 0.0430],\n",
      "          [0.0580, 0.0327, 0.0425,  ..., 0.0695, 0.0695, 0.0695],\n",
      "          ...,\n",
      "          [0.0349, 0.0490, 0.0395,  ..., 0.0713, 0.0713, 0.0713],\n",
      "          [0.0349, 0.0490, 0.0395,  ..., 0.0713, 0.0713, 0.0713],\n",
      "          [0.0349, 0.0490, 0.0395,  ..., 0.0713, 0.0713, 0.0713]],\n",
      "\n",
      "         [[0.0576, 0.0857, 0.0398,  ..., 0.0614, 0.0614, 0.0614],\n",
      "          [0.0397, 0.0450, 0.0764,  ..., 0.0453, 0.0453, 0.0453],\n",
      "          [0.0515, 0.0507, 0.0811,  ..., 0.0477, 0.0477, 0.0477],\n",
      "          ...,\n",
      "          [0.0502, 0.0447, 0.0417,  ..., 0.0548, 0.0548, 0.0548],\n",
      "          [0.0502, 0.0447, 0.0417,  ..., 0.0548, 0.0548, 0.0548],\n",
      "          [0.0502, 0.0447, 0.0417,  ..., 0.0548, 0.0548, 0.0548]],\n",
      "\n",
      "         [[0.1049, 0.0668, 0.0534,  ..., 0.0329, 0.0329, 0.0329],\n",
      "          [0.0405, 0.0526, 0.0524,  ..., 0.0376, 0.0376, 0.0376],\n",
      "          [0.0665, 0.0471, 0.0270,  ..., 0.0653, 0.0653, 0.0653],\n",
      "          ...,\n",
      "          [0.0340, 0.0305, 0.0535,  ..., 0.0574, 0.0574, 0.0574],\n",
      "          [0.0340, 0.0305, 0.0535,  ..., 0.0574, 0.0574, 0.0574],\n",
      "          [0.0340, 0.0305, 0.0535,  ..., 0.0574, 0.0574, 0.0574]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0662, 0.0312, 0.0576,  ..., 0.0607, 0.0607, 0.0607],\n",
      "          [0.0489, 0.0679, 0.0475,  ..., 0.0472, 0.0472, 0.0472],\n",
      "          [0.0514, 0.0501, 0.0316,  ..., 0.0417, 0.0417, 0.0417],\n",
      "          ...,\n",
      "          [0.0352, 0.0593, 0.0343,  ..., 0.0562, 0.0562, 0.0562],\n",
      "          [0.0352, 0.0593, 0.0343,  ..., 0.0562, 0.0562, 0.0562],\n",
      "          [0.0352, 0.0593, 0.0343,  ..., 0.0562, 0.0562, 0.0562]],\n",
      "\n",
      "         [[0.0404, 0.0600, 0.0793,  ..., 0.0393, 0.0393, 0.0393],\n",
      "          [0.0448, 0.0332, 0.0835,  ..., 0.0302, 0.0302, 0.0302],\n",
      "          [0.0331, 0.0352, 0.0497,  ..., 0.0612, 0.0612, 0.0612],\n",
      "          ...,\n",
      "          [0.0493, 0.0698, 0.0468,  ..., 0.0391, 0.0391, 0.0391],\n",
      "          [0.0493, 0.0698, 0.0468,  ..., 0.0391, 0.0391, 0.0391],\n",
      "          [0.0493, 0.0698, 0.0468,  ..., 0.0391, 0.0391, 0.0391]],\n",
      "\n",
      "         [[0.0163, 0.0410, 0.0344,  ..., 0.0636, 0.0636, 0.0636],\n",
      "          [0.0420, 0.0565, 0.0416,  ..., 0.0364, 0.0364, 0.0364],\n",
      "          [0.0434, 0.0555, 0.0448,  ..., 0.0556, 0.0556, 0.0556],\n",
      "          ...,\n",
      "          [0.0607, 0.0374, 0.0254,  ..., 0.0594, 0.0594, 0.0594],\n",
      "          [0.0607, 0.0374, 0.0254,  ..., 0.0594, 0.0594, 0.0594],\n",
      "          [0.0607, 0.0374, 0.0254,  ..., 0.0594, 0.0594, 0.0594]]]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([10, 8, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "# (B, num_heads, L, L)\n",
    "attn_dists = F.softmax(attn_scores, dim=-1) # (B, num_heads, L, L)\n",
    "\n",
    "print(attn_dists)\n",
    "print(attn_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "logical-period",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "attn_values = torch.matmul(attn_dists, v) # (B, num_heads, L, d_k)\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-titanium",
   "metadata": {},
   "source": [
    "## 각 head 의 결과물 병합\n",
    "\n",
    "각 head 의 결과물을 concat 하고 동일 차원으로 linear transformation 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pleasant-words",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
    "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "figured-kinase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0524, -0.0433,  0.0039,  ...,  0.0866, -0.0977,  0.0717],\n",
      "         [-0.0616, -0.1470,  0.1049,  ...,  0.0226, -0.1224,  0.0990],\n",
      "         [ 0.0364, -0.0454,  0.0514,  ...,  0.0789, -0.0852,  0.1245],\n",
      "         ...,\n",
      "         [ 0.0358,  0.0075,  0.0507,  ...,  0.0781, -0.1399,  0.1180],\n",
      "         [ 0.0358,  0.0075,  0.0507,  ...,  0.0781, -0.1399,  0.1180],\n",
      "         [ 0.0358,  0.0075,  0.0507,  ...,  0.0781, -0.1399,  0.1180]],\n",
      "\n",
      "        [[ 0.2211,  0.0014, -0.0457,  ...,  0.0663, -0.2574,  0.1505],\n",
      "         [ 0.1832, -0.0408, -0.0717,  ...,  0.0201, -0.2497,  0.1716],\n",
      "         [ 0.1919, -0.0275, -0.0324,  ...,  0.0249, -0.2271,  0.1705],\n",
      "         ...,\n",
      "         [ 0.2243, -0.0071, -0.0034,  ...,  0.0448, -0.2512,  0.1563],\n",
      "         [ 0.2243, -0.0071, -0.0034,  ...,  0.0448, -0.2512,  0.1563],\n",
      "         [ 0.2243, -0.0071, -0.0034,  ...,  0.0448, -0.2512,  0.1563]],\n",
      "\n",
      "        [[ 0.1840,  0.0386,  0.0989,  ..., -0.0412, -0.1912,  0.1261],\n",
      "         [ 0.1636,  0.0452,  0.0928,  ..., -0.0057, -0.1921,  0.1686],\n",
      "         [ 0.1522,  0.0343,  0.1166,  ...,  0.0343, -0.2041,  0.1083],\n",
      "         ...,\n",
      "         [ 0.1537,  0.0341,  0.1199,  ...,  0.0060, -0.1799,  0.1377],\n",
      "         [ 0.1537,  0.0341,  0.1199,  ...,  0.0060, -0.1799,  0.1377],\n",
      "         [ 0.1537,  0.0341,  0.1199,  ...,  0.0060, -0.1799,  0.1377]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0113, -0.1593, -0.1406,  ..., -0.0348, -0.0930,  0.0114],\n",
      "         [ 0.0366, -0.1602, -0.1883,  ...,  0.0094, -0.0021,  0.0214],\n",
      "         [ 0.0615, -0.1561, -0.1817,  ...,  0.0088, -0.0615,  0.0038],\n",
      "         ...,\n",
      "         [ 0.0425, -0.1529, -0.1573,  ...,  0.0652, -0.0410,  0.0048],\n",
      "         [ 0.0739, -0.1407, -0.1986,  ...,  0.0503, -0.0217, -0.0102],\n",
      "         [ 0.0296, -0.1312, -0.1655,  ...,  0.0091, -0.0658,  0.0117]],\n",
      "\n",
      "        [[ 0.1782, -0.0416,  0.0188,  ..., -0.0554, -0.1745,  0.1519],\n",
      "         [ 0.1747, -0.0068,  0.0388,  ..., -0.0598, -0.1732,  0.1052],\n",
      "         [ 0.1601, -0.0414,  0.0856,  ..., -0.0742, -0.1693,  0.1006],\n",
      "         ...,\n",
      "         [ 0.1944, -0.0117,  0.1349,  ..., -0.0245, -0.1571,  0.1226],\n",
      "         [ 0.1944, -0.0117,  0.1349,  ..., -0.0245, -0.1571,  0.1226],\n",
      "         [ 0.1944, -0.0117,  0.1349,  ..., -0.0245, -0.1571,  0.1226]],\n",
      "\n",
      "        [[ 0.0190, -0.0099, -0.0150,  ..., -0.0645, -0.1472, -0.0214],\n",
      "         [ 0.0911,  0.0220, -0.0361,  ..., -0.0892, -0.1348, -0.0496],\n",
      "         [ 0.1297, -0.0026, -0.0791,  ..., -0.0334, -0.1374,  0.0047],\n",
      "         ...,\n",
      "         [ 0.1254,  0.0181,  0.0101,  ..., -0.0598, -0.1645,  0.0278],\n",
      "         [ 0.1254,  0.0181,  0.0101,  ..., -0.0598, -0.1645,  0.0278],\n",
      "         [ 0.1254,  0.0181,  0.0101,  ..., -0.0598, -0.1645,  0.0278]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "outputs = w_0(attn_values)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-session",
   "metadata": {},
   "source": [
    "## 전체코드\n",
    "\n",
    "하나의 Multi-head attention 모듈 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "electronic-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "\n",
    "        # Q, K, V learnable matrices\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Linear transformation for concatenated outputs\n",
    "        self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        q = self.w_q(q)  # (B, L, d_model)\n",
    "        k = self.w_k(k)  # (B, L, d_model)\n",
    "        v = self.w_v(v)  # (B, L, d_model)\n",
    "\n",
    "        q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "        k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "        v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "        q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "        k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "        v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "        attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
    "        attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
    "\n",
    "        return self.w_0(attn_values)\n",
    "\n",
    "    def self_attention(self, q, k, v):\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
    "        attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "        attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "        return attn_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "documentary-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = MultiheadAttention()\n",
    "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)\n",
    "# (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fundamental-exposure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0924, -0.1833, -0.0233,  ..., -0.0078, -0.2165,  0.1387],\n",
      "         [-0.0651, -0.1497, -0.0011,  ..., -0.0165, -0.2354,  0.1337],\n",
      "         [-0.0770, -0.1244, -0.0269,  ...,  0.0083, -0.2136,  0.1760],\n",
      "         ...,\n",
      "         [-0.0547, -0.1334, -0.0058,  ...,  0.0088, -0.2223,  0.1259],\n",
      "         [-0.0547, -0.1334, -0.0058,  ...,  0.0088, -0.2223,  0.1259],\n",
      "         [-0.0547, -0.1334, -0.0058,  ...,  0.0088, -0.2223,  0.1259]],\n",
      "\n",
      "        [[ 0.0800, -0.2903, -0.0331,  ...,  0.0686,  0.0250,  0.3676],\n",
      "         [ 0.1047, -0.2659,  0.0068,  ...,  0.0529,  0.0105,  0.3308],\n",
      "         [ 0.1233, -0.2852,  0.0123,  ...,  0.0626,  0.0367,  0.3798],\n",
      "         ...,\n",
      "         [ 0.1296, -0.2597,  0.0376,  ...,  0.0455,  0.0120,  0.3828],\n",
      "         [ 0.1296, -0.2597,  0.0376,  ...,  0.0455,  0.0120,  0.3828],\n",
      "         [ 0.1296, -0.2597,  0.0376,  ...,  0.0455,  0.0120,  0.3828]],\n",
      "\n",
      "        [[ 0.0602, -0.2527,  0.0954,  ...,  0.0286, -0.1102,  0.2559],\n",
      "         [ 0.1172, -0.2607,  0.0771,  ...,  0.0224, -0.1259,  0.2402],\n",
      "         [ 0.0818, -0.2386,  0.0716,  ...,  0.0468, -0.1653,  0.2632],\n",
      "         ...,\n",
      "         [ 0.1247, -0.2498,  0.0360,  ...,  0.0555, -0.1314,  0.2748],\n",
      "         [ 0.1247, -0.2498,  0.0360,  ...,  0.0555, -0.1314,  0.2748],\n",
      "         [ 0.1247, -0.2498,  0.0360,  ...,  0.0555, -0.1314,  0.2748]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1597, -0.0068,  0.1443,  ...,  0.0620,  0.1783, -0.0381],\n",
      "         [-0.1840, -0.0045,  0.0794,  ...,  0.0263,  0.1600, -0.0409],\n",
      "         [-0.1714, -0.0308,  0.0964,  ...,  0.0507,  0.1976, -0.0539],\n",
      "         ...,\n",
      "         [-0.1431, -0.0442,  0.0746,  ...,  0.0151,  0.1534, -0.0802],\n",
      "         [-0.1445, -0.0353,  0.0847,  ...,  0.0311,  0.1779, -0.0339],\n",
      "         [-0.1163, -0.0195,  0.1024,  ...,  0.0350,  0.1825, -0.0273]],\n",
      "\n",
      "        [[-0.1117, -0.1947,  0.0138,  ..., -0.0331,  0.0045,  0.2921],\n",
      "         [-0.1184, -0.1182,  0.0681,  ...,  0.0466,  0.0106,  0.3341],\n",
      "         [-0.1111, -0.1427,  0.0226,  ..., -0.0534,  0.0197,  0.2583],\n",
      "         ...,\n",
      "         [-0.0572, -0.1467,  0.0288,  ..., -0.0212, -0.0438,  0.3102],\n",
      "         [-0.0572, -0.1467,  0.0288,  ..., -0.0212, -0.0438,  0.3102],\n",
      "         [-0.0572, -0.1467,  0.0288,  ..., -0.0212, -0.0438,  0.3102]],\n",
      "\n",
      "        [[-0.0612, -0.2723,  0.1382,  ..., -0.0005,  0.1221,  0.2931],\n",
      "         [-0.0962, -0.2717,  0.1531,  ...,  0.0729,  0.0981,  0.2931],\n",
      "         [-0.0414, -0.2800,  0.1271,  ...,  0.0015,  0.0994,  0.3545],\n",
      "         ...,\n",
      "         [-0.0562, -0.2597,  0.1646,  ...,  0.0727,  0.1267,  0.3235],\n",
      "         [-0.0562, -0.2597,  0.1646,  ...,  0.0727,  0.1267,  0.3235],\n",
      "         [-0.0562, -0.2597,  0.1646,  ...,  0.0727,  0.1267,  0.3235]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-environment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
