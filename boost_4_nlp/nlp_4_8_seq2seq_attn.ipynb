{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "generous-insulation",
   "metadata": {},
   "source": [
    "# Seq2seq + Attention\n",
    "\n",
    "1. 여러 Attention 모듈을 구현합니다.\n",
    "2. 기존 Seq2seq 모델과의 차이를 이해합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-equilibrium",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expensive-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-branch",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "데이터 처리는 이전과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hollywood-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "pad_id = 0\n",
    "sos_id = 1\n",
    "eos_id = 2\n",
    "\n",
    "src_data = [\n",
    "  [3, 77, 56, 26, 3, 55, 12, 36, 31],\n",
    "  [58, 20, 65, 46, 26, 10, 76, 44],\n",
    "  [58, 17, 8],\n",
    "  [59],\n",
    "  [29, 3, 52, 74, 73, 51, 39, 75, 19],\n",
    "  [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93],\n",
    "  [39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99, 5],\n",
    "  [75, 34, 17, 3, 86, 88],\n",
    "  [63, 39, 5, 35, 67, 56, 68, 89, 55, 66],\n",
    "  [12, 40, 69, 39, 49]\n",
    "]\n",
    "\n",
    "trg_data = [\n",
    "  [75, 13, 22, 77, 89, 21, 13, 86, 95],\n",
    "  [79, 14, 91, 41, 32, 79, 88, 34, 8, 68, 32, 77, 58, 7, 9, 87],\n",
    "  [85, 8, 50, 30],\n",
    "  [47, 30],\n",
    "  [8, 85, 87, 77, 47, 21, 23, 98, 83, 4, 47, 97, 40, 43, 70, 8, 65, 71, 69, 88],\n",
    "  [32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18],\n",
    "  [37, 14, 49, 24, 93, 37, 54, 51, 39, 84],\n",
    "  [16, 98, 68, 57, 55, 46, 66, 85, 18],\n",
    "  [20, 70, 14, 6, 58, 90, 30, 17, 91, 18, 90],\n",
    "  [37, 93, 98, 13, 45, 28, 89, 72, 70]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "technical-console",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 143150.31it/s]\n"
     ]
    }
   ],
   "source": [
    "trg_data = [[sos_id]+seq+[eos_id] for seq in tqdm(trg_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "victorian-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(data, is_src=True):\n",
    "    max_len = len(max(data, key=len))\n",
    "    print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "    valid_lens = []\n",
    "    for i, seq in enumerate(tqdm(data)):\n",
    "        valid_lens.append(len(seq))\n",
    "        if len(seq) < max_len:\n",
    "            data[i] = seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "    return data, valid_lens, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "victorian-insight",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 107546.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 133152.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 15\n",
      "Maximum sequence length: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "src_data, src_lens, src_max_len = padding(src_data)\n",
    "trg_data, trg_lens, trg_max_len = padding(trg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "standing-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 15])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 22])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# B: batch size, S_L: source maximum sequence length, T_L: target maximum sequence length\n",
    "src_batch = torch.LongTensor(src_data)  # (B, S_L)\n",
    "src_batch_lens = torch.LongTensor(src_lens)  # (B)\n",
    "trg_batch = torch.LongTensor(trg_data)  # (B, T_L)\n",
    "trg_batch_lens = torch.LongTensor(trg_lens)  # (B)\n",
    "\n",
    "print(src_batch.shape)\n",
    "print(src_batch_lens.shape)\n",
    "print(trg_batch.shape)\n",
    "print(trg_batch_lens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "balanced-effect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99,  5],\n",
      "        [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93,  0,  0,  0,  0],\n",
      "        [63, 39,  5, 35, 67, 56, 68, 89, 55, 66,  0,  0,  0,  0,  0],\n",
      "        [ 3, 77, 56, 26,  3, 55, 12, 36, 31,  0,  0,  0,  0,  0,  0],\n",
      "        [29,  3, 52, 74, 73, 51, 39, 75, 19,  0,  0,  0,  0,  0,  0],\n",
      "        [58, 20, 65, 46, 26, 10, 76, 44,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [75, 34, 17,  3, 86, 88,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12, 40, 69, 39, 49,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [58, 17,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [59,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([15, 11, 10,  9,  9,  8,  6,  5,  3,  1])\n",
      "tensor([[ 1, 37, 14, 49, 24, 93, 37, 54, 51, 39, 84,  2,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18,  2,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 20, 70, 14,  6, 58, 90, 30, 17, 91, 18, 90,  2,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 75, 13, 22, 77, 89, 21, 13, 86, 95,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1,  8, 85, 87, 77, 47, 21, 23, 98, 83,  4, 47, 97, 40, 43, 70,  8, 65,\n",
      "         71, 69, 88,  2],\n",
      "        [ 1, 79, 14, 91, 41, 32, 79, 88, 34,  8, 68, 32, 77, 58,  7,  9, 87,  2,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 16, 98, 68, 57, 55, 46, 66, 85, 18,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 37, 93, 98, 13, 45, 28, 89, 72, 70,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 85,  8, 50, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 47, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0]])\n",
      "tensor([12, 14, 13, 11, 22, 18, 11, 11,  6,  4])\n"
     ]
    }
   ],
   "source": [
    "src_batch_lens, sorted_idx = src_batch_lens.sort(descending=True)\n",
    "src_batch = src_batch[sorted_idx]\n",
    "trg_batch = trg_batch[sorted_idx]\n",
    "trg_batch_lens = trg_batch_lens[sorted_idx]\n",
    "\n",
    "print(src_batch)\n",
    "print(src_batch_lens)\n",
    "print(trg_batch)\n",
    "print(trg_batch_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-employer",
   "metadata": {},
   "source": [
    "## Encoder 구현\n",
    "\n",
    "이전 Seq2seq 와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pacific-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "num_dirs = 2\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exotic-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_size, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True if num_dirs > 1 else False,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.linear = nn.Linear(num_dirs * hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, batch, batch_lens):  # batch: (B, S_L), batch_lens: (B)\n",
    "        # d_w: word embedding size\n",
    "        batch_emb = self.embedding(batch)  # (B, S_L, d_w)\n",
    "        batch_emb = batch_emb.transpose(0, 1)  # (S_L, B, d_w)\n",
    "\n",
    "        packed_input = pack_padded_sequence(batch_emb, batch_lens)\n",
    "\n",
    "        h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size))  # (num_layers*num_dirs, B, d_h) = (4, B, d_h)\n",
    "        packed_outputs, h_n = self.gru(packed_input, h_0)  # h_n: (4, B, d_h)\n",
    "        outputs = pad_packed_sequence(packed_outputs)[0]  # outputs: (S_L, B, 2d_h)\n",
    "        outputs = torch.tanh(self.linear(outputs))  # (S_L, B, d_h)\n",
    "\n",
    "        forward_hidden = h_n[-2, :, :]\n",
    "        backward_hidden = h_n[-1, :, :]\n",
    "        hidden = torch.tanh(self.linear(torch.cat((forward_hidden, backward_hidden), dim=-1))).unsqueeze(0)  # (1, B, d_h)\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "artistic-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-penny",
   "metadata": {},
   "source": [
    "## Dot-product Attention 구현\n",
    "\n",
    "대표적인 Attention 형태 중 하나인 Dot-product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "overall-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs): \n",
    "        #(1,B,d_h), (S_L, B, d_h)\n",
    "        query = decoder_hidden.squeeze(0) # (B, d_h)\n",
    "        key = encoder_outputs.transpose(0,1) # (B, S_L, d_h)\n",
    "        \n",
    "        energy = torch.sum(torch.mul(key, query.unsqueeze(1)), dim=-1)\n",
    "        # (B, S_L)\n",
    "        \n",
    "        attn_scores = F.softmax(energy, dim=-1) # (B, S_L)\n",
    "        attn_values = torch.sum(torch.mul(encoder_outputs.transpose(0,1),\n",
    "                                          attn_scores.unsqueeze(2)), dim=1)\n",
    "        # (B, d_h)\n",
    "        \n",
    "        return attn_values, attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unable-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_attn = DotAttention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-means",
   "metadata": {},
   "source": [
    "이제 이 attention 모듈을 가지는 Decoder 클래스를 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "accessory-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, attention): # attention\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.attention = attention\n",
    "        self.rnn = nn.GRU(\n",
    "             embedding_size,\n",
    "             hidden_size\n",
    "        )\n",
    "        self.output_linear = nn.Linear(2*hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, batch, encoder_outputs, hidden): \n",
    "        # batch: (B), encoder_outputs : (L, B, d_h), hidden : (1, B, d_h)\n",
    "        batch_emb = self.embedding(batch)\n",
    "        batch_emb = batch_emb.unsqueeze(0) # (1, B, d_w)\n",
    "        \n",
    "        outputs, hidden = self.rnn(batch_emb, hidden)\n",
    "        # (1, B, d_h) , (1, B, d_h)\n",
    "        \n",
    "        attn_values, attn_scores = self.attention(hidden, encoder_outputs)\n",
    "        # (B, d_h), (B, S_L)\n",
    "        concat_outputs = torch.cat((outputs, attn_values.unsqueeze(0)), dim = -1) \n",
    "        # (1, B, 2d_h)\n",
    "        return self.output_linear(concat_outputs).squeeze(0), hidden\n",
    "        # (B, V), (1, B, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "handy-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(dot_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-montreal",
   "metadata": {},
   "source": [
    "## Seq2seq 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mysterious-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src_batch, src_batch_lens, trg_batch, teacher_forcing_prob=0.5):\n",
    "    # src_batch: (B, S_L), src_batch_lens: (B), trg_batch: (B, T_L)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_batch_lens)  # encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)\n",
    "\n",
    "        input_ids = trg_batch[:, 0]  # (B)\n",
    "        batch_size = src_batch.shape[0]\n",
    "        outputs = torch.zeros(trg_max_len, batch_size, vocab_size)  # (T_L, B, V)\n",
    "\n",
    "        for t in range(1, trg_max_len):\n",
    "            decoder_outputs, hidden = self.decoder(input_ids, encoder_outputs, hidden)  # decoder_outputs: (B, V), hidden: (1, B, d_h)\n",
    "\n",
    "            outputs[t] = decoder_outputs\n",
    "            _, top_ids = torch.max(decoder_outputs, dim=-1)  # top_ids: (B)\n",
    "\n",
    "            input_ids = trg_batch[:, t] if random.random() > teacher_forcing_prob else top_ids\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bacterial-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-tobago",
   "metadata": {},
   "source": [
    "## 모델 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stone-criminal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.1856e-01,  1.0816e-01,  1.3034e-02,  ..., -1.4176e-01,\n",
      "           1.0259e-01,  1.6374e-01],\n",
      "         [ 1.4788e-01,  1.1311e-01,  1.9207e-04,  ..., -1.1806e-01,\n",
      "           1.0833e-01,  1.1973e-01],\n",
      "         [ 1.6875e-01,  6.9487e-02,  1.4208e-02,  ..., -1.2804e-01,\n",
      "           9.5588e-02,  1.6005e-01],\n",
      "         ...,\n",
      "         [ 1.2274e-01,  1.0750e-01,  1.5686e-04,  ..., -1.3818e-01,\n",
      "           8.6787e-02,  1.0720e-01],\n",
      "         [ 1.1707e-01,  9.0710e-02, -4.0094e-03,  ..., -1.2709e-01,\n",
      "           1.1689e-01,  1.1094e-01],\n",
      "         [ 1.2660e-01,  1.2850e-01,  2.0520e-02,  ..., -1.2909e-01,\n",
      "           1.2924e-01,  1.2882e-01]],\n",
      "\n",
      "        [[ 9.2474e-02,  1.1941e-02,  1.2804e-01,  ..., -2.0181e-01,\n",
      "           1.6475e-02,  1.4646e-01],\n",
      "         [ 6.1336e-02,  9.3359e-02, -3.1194e-02,  ..., -1.0289e-01,\n",
      "           4.3983e-02,  1.7623e-01],\n",
      "         [ 1.4851e-01,  5.2601e-02,  4.1210e-02,  ..., -9.1149e-02,\n",
      "           3.7108e-02,  1.9207e-02],\n",
      "         ...,\n",
      "         [ 9.6398e-02,  1.8619e-02,  1.1294e-01,  ..., -2.0354e-01,\n",
      "           9.9058e-03,  1.1494e-01],\n",
      "         [ 1.6951e-03,  5.3716e-02, -2.3670e-02,  ..., -1.0871e-01,\n",
      "           5.7251e-02,  6.5131e-02],\n",
      "         [ 1.7202e-01, -2.6058e-02,  6.7537e-02,  ..., -1.0872e-01,\n",
      "           9.0305e-02,  4.7526e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.6273e-01, -1.6550e-01, -1.9575e-01,  ...,  4.1843e-02,\n",
      "           3.5737e-03,  1.4128e-01],\n",
      "         [-3.5920e-02, -2.1020e-01, -1.6009e-01,  ..., -4.9131e-02,\n",
      "          -3.1579e-02,  1.1879e-01],\n",
      "         [-5.4720e-02, -2.4352e-01, -1.2865e-01,  ..., -5.2327e-02,\n",
      "          -2.6604e-02,  1.5934e-01],\n",
      "         ...,\n",
      "         [-6.7868e-02, -2.1522e-01, -1.3448e-01,  ..., -4.7388e-02,\n",
      "          -2.9755e-02,  1.3508e-01],\n",
      "         [-9.3639e-02, -1.2293e-01, -2.3736e-01,  ..., -1.5626e-02,\n",
      "           4.2221e-02,  1.0886e-01],\n",
      "         [-7.9715e-02, -1.9959e-01, -2.5620e-01,  ...,  3.1468e-02,\n",
      "          -2.0142e-02,  1.2231e-01]],\n",
      "\n",
      "        [[-1.6457e-01, -1.9996e-01, -2.6967e-01,  ...,  3.9920e-02,\n",
      "          -4.0430e-03,  1.3693e-01],\n",
      "         [-7.5510e-02, -2.1385e-01, -2.6802e-01,  ..., -2.9649e-03,\n",
      "          -2.2284e-02,  1.0497e-01],\n",
      "         [-8.8578e-02, -2.4394e-01, -2.3491e-01,  ..., -1.1958e-02,\n",
      "          -1.6719e-02,  1.4714e-01],\n",
      "         ...,\n",
      "         [-1.0127e-01, -2.1402e-01, -2.3958e-01,  ..., -9.3670e-03,\n",
      "          -2.0603e-02,  1.2462e-01],\n",
      "         [-1.2129e-01, -1.7064e-01, -2.8002e-01,  ...,  1.2856e-03,\n",
      "           2.1137e-02,  1.1401e-01],\n",
      "         [-1.0836e-01, -2.0936e-01, -2.9305e-01,  ...,  3.3010e-02,\n",
      "          -1.3961e-02,  1.3219e-01]],\n",
      "\n",
      "        [[-9.2049e-02, -2.1306e-02, -1.6778e-01,  ..., -6.0820e-03,\n",
      "           9.2996e-02,  1.0979e-01],\n",
      "         [-2.0864e-02, -1.5409e-02, -1.9110e-01,  ..., -2.3671e-02,\n",
      "           8.9909e-02,  7.1287e-02],\n",
      "         [-3.9130e-02, -2.3644e-01,  7.6853e-02,  ..., -1.0295e-01,\n",
      "          -2.0828e-02,  1.7818e-01],\n",
      "         ...,\n",
      "         [-4.4235e-02, -1.5135e-02, -1.6559e-01,  ..., -3.5227e-02,\n",
      "           9.2816e-02,  9.0543e-02],\n",
      "         [-6.9175e-02, -1.8280e-01,  7.8131e-02,  ..., -9.7756e-02,\n",
      "          -3.1741e-03,  1.5840e-01],\n",
      "         [-7.3141e-02, -1.3988e-01, -2.3729e-01,  ..., -6.5288e-02,\n",
      "          -2.0849e-01,  1.0406e-01]]], grad_fn=<CopySlices>)\n",
      "torch.Size([22, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "# V: vocab size\n",
    "outputs = seq2seq(src_batch, src_batch_lens, trg_batch)  # (T_L, B, V)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "juvenile-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sent = [4, 10, 88, 46, 72, 34, 14, 51]\n",
    "sample_len = len(sample_sent)\n",
    "\n",
    "sample_batch = torch.LongTensor(sample_sent).unsqueeze(0)  # (1, L)\n",
    "sample_batch_len = torch.LongTensor([sample_len])  # (1)\n",
    "\n",
    "encoder_output, hidden = seq2seq.encoder(sample_batch, sample_batch_len)  # hidden: (4, 1, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "prime-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id = torch.LongTensor([sos_id]) # (1)\n",
    "output = []\n",
    "\n",
    "for t in range(1, trg_max_len):\n",
    "    decoder_output, hidden = seq2seq.decoder(input_id, encoder_output, hidden)  # decoder_output: (1, V), hidden: (4, 1, d_h)\n",
    "\n",
    "    _, top_id = torch.max(decoder_output, dim=-1)  # top_ids: (1)\n",
    "\n",
    "    if top_id == eos_id:\n",
    "        break\n",
    "    else:\n",
    "        output += top_id.tolist()\n",
    "        input_id = top_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "about-order",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88, 98, 24, 74, 12, 96, 73, 13, 13, 23, 94, 62, 72, 56, 13, 13, 94, 61, 80, 80, 25]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-abortion",
   "metadata": {},
   "source": [
    "## Concat Attention 구현\n",
    "\n",
    "Bahdanau Attention 이라고도 불리는 Concat Attention 을 구현해보자\n",
    "* `self.w` : Concat 한 query 와 key 벡터를 1차적으로 linear transformation\n",
    "* `self.v` : Attention logit 값을 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "interesting-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w = nn.Linear(2*hidden_size, hidden_size, bias=False)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):  # (1, B, d_h), (S_L, B, d_h)\n",
    "        src_max_len = encoder_outputs.shape[0]\n",
    "\n",
    "        decoder_hidden = decoder_hidden.transpose(0, 1).repeat(1, src_max_len, 1)  # (B, S_L, d_h)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # (B, S_L, d_h)\n",
    "\n",
    "        concat_hiddens = torch.cat((decoder_hidden, encoder_outputs), dim=2)  # (B, S_L, 2d_h)\n",
    "        energy = torch.tanh(self.w(concat_hiddens))  # (B, S_L, d_h)\n",
    "\n",
    "        attn_scores = F.softmax(self.v(energy), dim=1)  # (B, S_L, 1)\n",
    "        attn_values = torch.sum(torch.mul(encoder_outputs, attn_scores), dim=1)  # (B, d_h)\n",
    "\n",
    "        return attn_values, attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "public-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_attn = ConcatAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "charged-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.attention = attention\n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_size + hidden_size,\n",
    "            hidden_size\n",
    "        )\n",
    "        self.output_linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, batch, encoder_outputs, hidden):  # batch: (B), encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)  \n",
    "        batch_emb = self.embedding(batch)  # (B, d_w)\n",
    "        batch_emb = batch_emb.unsqueeze(0)  # (1, B, d_w)\n",
    "\n",
    "        attn_values, attn_scores = self.attention(hidden, encoder_outputs)  # (B, d_h), (B, S_L)\n",
    "\n",
    "        concat_emb = torch.cat((batch_emb, attn_values.unsqueeze(0)), dim=-1)  # (1, B, d_w+d_h)\n",
    "\n",
    "        outputs, hidden = self.rnn(concat_emb, hidden)  # (1, B, d_h), (1, B, d_h)\n",
    "\n",
    "        return self.output_linear(outputs).squeeze(0), hidden  # (B, V), (1, B, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "established-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(concat_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "mechanical-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "partial-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0030,  0.0956, -0.1299,  ..., -0.0355, -0.0080, -0.0551],\n",
      "         [ 0.0432,  0.0365, -0.1335,  ...,  0.0048,  0.0124, -0.0232],\n",
      "         [ 0.0764,  0.0957, -0.1184,  ..., -0.0396,  0.0017, -0.0469],\n",
      "         ...,\n",
      "         [ 0.0333,  0.0686, -0.1385,  ..., -0.0049, -0.0621, -0.0433],\n",
      "         [ 0.0160,  0.1058, -0.1270,  ..., -0.0233,  0.0085, -0.0530],\n",
      "         [ 0.0372,  0.0892, -0.1486,  ..., -0.0234,  0.0236, -0.0206]],\n",
      "\n",
      "        [[ 0.0332,  0.1695, -0.2200,  ..., -0.0126,  0.0953, -0.0379],\n",
      "         [ 0.0662,  0.1333, -0.2168,  ...,  0.0064,  0.1128, -0.0262],\n",
      "         [ 0.0756,  0.1584, -0.2049,  ..., -0.0160,  0.1130, -0.0417],\n",
      "         ...,\n",
      "         [ 0.0534,  0.1388, -0.2171,  ..., -0.0013,  0.0754, -0.0304],\n",
      "         [ 0.0502,  0.1658, -0.1977,  ..., -0.0135,  0.1143, -0.0361],\n",
      "         [ 0.0596,  0.1614, -0.2212,  ..., -0.0192,  0.1260, -0.0196]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3046,  0.0484, -0.2000,  ..., -0.2551,  0.0129, -0.0300],\n",
      "         [ 0.3036,  0.0074, -0.2067,  ..., -0.2317,  0.0066, -0.0690],\n",
      "         [ 0.3232,  0.0314, -0.1998,  ..., -0.2172,  0.0231, -0.0521],\n",
      "         ...,\n",
      "         [ 0.2819, -0.0563, -0.1800,  ..., -0.2806, -0.0910, -0.0438],\n",
      "         [ 0.3114,  0.0358, -0.1910,  ..., -0.2624,  0.0218, -0.0308],\n",
      "         [ 0.3158,  0.0340, -0.1887,  ..., -0.2579,  0.0204, -0.0277]],\n",
      "\n",
      "        [[ 0.4060,  0.0216, -0.2130,  ..., -0.3194, -0.0732, -0.0668],\n",
      "         [ 0.4105, -0.0055, -0.2229,  ..., -0.2982, -0.0740, -0.0997],\n",
      "         [ 0.4247, -0.0010, -0.2080,  ..., -0.2902, -0.0642, -0.0888],\n",
      "         ...,\n",
      "         [ 0.3910, -0.0429, -0.1932,  ..., -0.3189, -0.1317, -0.0657],\n",
      "         [ 0.4118,  0.0086, -0.2045,  ..., -0.3248, -0.0647, -0.0708],\n",
      "         [ 0.4157,  0.0071, -0.2026,  ..., -0.3204, -0.0650, -0.0696]],\n",
      "\n",
      "        [[ 0.3107,  0.1168, -0.1379,  ..., -0.1118,  0.0831, -0.0499],\n",
      "         [ 0.3220,  0.0976, -0.1441,  ..., -0.0949,  0.0844, -0.0753],\n",
      "         [ 0.3290,  0.0912, -0.1307,  ..., -0.0858,  0.0931, -0.0660],\n",
      "         ...,\n",
      "         [ 0.2929,  0.0681, -0.1101,  ..., -0.1062,  0.0567, -0.0249],\n",
      "         [ 0.3135,  0.1042, -0.1235,  ..., -0.1171,  0.0909, -0.0537],\n",
      "         [ 0.3168,  0.1027, -0.1226,  ..., -0.1142,  0.0919, -0.0534]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "torch.Size([22, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "outputs = seq2seq(src_batch, src_batch_lens, trg_batch)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-blind",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
